{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Taxi Driver \n",
    "One of the tutorials recommended on open-ai gym docs. Instructions found here https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radom walk to taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001B[35m\u001B[34;1m\u001B[43mR\u001B[0m\u001B[0m\u001B[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 460\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n",
      "Timesteps taken: 460\n",
      "Penalties incurred: 155\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = []  # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "\n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "    }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "\n",
    "\n",
    "print_frames(frames)\n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 50.1 s, sys: 12.8 s, total: 1min 2s\n",
      "Wall time: 53.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.40613305,  -2.27325184,  -2.40927662,  -2.35794371,\n",
       "       -10.6115015 , -10.91907938])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 13.26\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ray[rllib]\n",
      "  Downloading ray-1.13.0-cp39-cp39-macosx_10_15_x86_64.whl (56.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.9/56.9 MB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: attrs in /Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages (from ray[rllib]) (21.4.0)\n",
      "Requirement already satisfied: jsonschema in /Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages (from ray[rllib]) (4.4.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0\n",
      "  Downloading msgpack-1.0.4-cp39-cp39-macosx_10_9_x86_64.whl (75 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m75.3/75.3 kB\u001B[0m \u001B[31m1.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting virtualenv\n",
      "  Downloading virtualenv-20.15.1-py2.py3-none-any.whl (10.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m4.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m\n",
      "\u001B[?25hCollecting grpcio<=1.43.0,>=1.28.1\n",
      "  Using cached grpcio-1.43.0-cp39-cp39-macosx_10_10_x86_64.whl (4.2 MB)\n",
      "Collecting click<=8.0.4,>=7.0\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m97.5/97.5 kB\u001B[0m \u001B[31m3.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.19.3 in /Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages (from ray[rllib]) (1.23.0)\n",
      "Collecting frozenlist\n",
      "  Using cached frozenlist-1.3.0-cp39-cp39-macosx_10_9_x86_64.whl (36 kB)\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-6.0-cp39-cp39-macosx_10_9_x86_64.whl (197 kB)\n",
      "Collecting protobuf<4.0.0,>=3.15.3\n",
      "  Downloading protobuf-3.20.1-cp39-cp39-macosx_10_9_x86_64.whl (962 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m962.4/962.4 kB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting aiosignal\n",
      "  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.8/62.8 kB\u001B[0m \u001B[31m2.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting scipy\n",
      "  Downloading scipy-1.8.1-cp39-cp39-macosx_12_0_universal2.macosx_10_9_x86_64.whl (55.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.6/55.6 MB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting lz4\n",
      "  Downloading lz4-4.0.1-cp39-cp39-macosx_10_9_x86_64.whl (90 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m90.5/90.5 kB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mta \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting tensorboardX>=1.9\n",
      "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m125.4/125.4 kB\u001B[0m \u001B[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting scikit-image\n",
      "  Downloading scikit_image-0.19.3-cp39-cp39-macosx_10_13_x86_64.whl (13.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.3/13.3 MB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: gym<0.22 in /Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages (from ray[rllib]) (0.21.0)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.7-cp39-cp39-macosx_10_9_x86_64.whl (109 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m109.5/109.5 kB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tabulate\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Collecting matplotlib!=3.4.3\n",
      "  Downloading matplotlib-3.5.2-cp39-cp39-macosx_10_9_x86_64.whl (7.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.3/7.3 MB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting pandas\n",
      "  Downloading pandas-1.4.3-cp39-cp39-macosx_10_9_x86_64.whl (11.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.5/11.5 MB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: six>=1.5.2 in /Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages (from grpcio<=1.43.0,>=1.28.1->ray[rllib]) (1.16.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages (from gym<0.22->ray[rllib]) (2.1.0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m930.9/930.9 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: packaging>=20.0 in /Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib]) (21.3)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.2.0-cp39-cp39-macosx_10_10_x86_64.whl (3.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/3.1 MB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pyparsing>=2.2.1 in /Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib]) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib]) (2.8.2)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.3-cp39-cp39-macosx_10_9_x86_64.whl (65 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m65.3/65.3 kB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages (from jsonschema->ray[rllib]) (0.18.0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m160.2/160.2 kB\u001B[0m \u001B[31m3.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "Collecting imageio>=2.4.1\n",
      "  Downloading imageio-2.19.3-py3-none-any.whl (3.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.3.0-cp39-cp39-macosx_10_13_x86_64.whl (4.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.4/4.4 MB\u001B[0m \u001B[31m4.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting networkx>=2.2\n",
      "  Downloading networkx-2.8.4-py3-none-any.whl (2.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2022.5.4-py3-none-any.whl (195 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m195.6/195.6 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting distlib<1,>=0.3.1\n",
      "  Using cached distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
      "Collecting platformdirs<3,>=2\n",
      "  Using cached platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pytz, msgpack, dm-tree, distlib, urllib3, tifffile, tabulate, scipy, pyyaml, PyWavelets, protobuf, platformdirs, pillow, networkx, lz4, kiwisolver, idna, grpcio, frozenlist, fonttools, filelock, cycler, click, charset-normalizer, certifi, virtualenv, tensorboardX, requests, pandas, matplotlib, imageio, aiosignal, scikit-image, ray\n",
      "Successfully installed PyWavelets-1.3.0 aiosignal-1.2.0 certifi-2022.6.15 charset-normalizer-2.1.0 click-8.0.4 cycler-0.11.0 distlib-0.3.4 dm-tree-0.1.7 filelock-3.7.1 fonttools-4.33.3 frozenlist-1.3.0 grpcio-1.43.0 idna-3.3 imageio-2.19.3 kiwisolver-1.4.3 lz4-4.0.1 matplotlib-3.5.2 msgpack-1.0.4 networkx-2.8.4 pandas-1.4.3 pillow-9.2.0 platformdirs-2.5.2 protobuf-3.20.1 pytz-2022.1 pyyaml-6.0 ray-1.13.0 requests-2.28.1 scikit-image-0.19.3 scipy-1.8.1 tabulate-0.8.10 tensorboardX-2.5.1 tifffile-2022.5.4 urllib3-1.26.9 virtualenv-20.15.1\n"
     ]
    }
   ],
   "source": [
    "# !conda install -c conda-forge ray-rllib #Linux\n",
    "\n",
    "!pip install 'ray[rllib]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "PyTorch was specified as the 'framework' inside of your config dictionary. However, there was no installation found. You can install PyTorch via `pip install torch`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 27>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m# Environment (RLlib understands openAI gym registered strings).\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTaxi-v3\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     23\u001B[0m     },\n\u001B[1;32m     24\u001B[0m }\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Create our RLlib Trainer.\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mPPOTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# Run it for n training iterations. A training iteration includes\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# parallel sample collection by the environment workers as well as\u001B[39;00m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# loss calculation on the collected batch and a model update.\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m3\u001B[39m):\n",
      "File \u001B[0;32m/Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:870\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001B[0m\n\u001B[1;32m    858\u001B[0m \u001B[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001B[39;00m\n\u001B[1;32m    859\u001B[0m \u001B[38;5;66;03m# available. We want to make sure the metrics are always present\u001B[39;00m\n\u001B[1;32m    860\u001B[0m \u001B[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001B[39;00m\n\u001B[1;32m    861\u001B[0m \u001B[38;5;66;03m# when we use these as stopping criteria.\u001B[39;00m\n\u001B[1;32m    862\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation_metrics \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    863\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m    864\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepisode_reward_max\u001B[39m\u001B[38;5;124m\"\u001B[39m: np\u001B[38;5;241m.\u001B[39mnan,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    867\u001B[0m     }\n\u001B[1;32m    868\u001B[0m }\n\u001B[0;32m--> 870\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    871\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogger_creator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremote_checkpoint_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msync_function_tpl\u001B[49m\n\u001B[1;32m    872\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages/ray/tune/trainable.py:156\u001B[0m, in \u001B[0;36mTrainable.__init__\u001B[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001B[0m\n\u001B[1;32m    154\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_local_ip \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_current_ip()\n\u001B[0;32m--> 156\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeepcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m setup_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m setup_time \u001B[38;5;241m>\u001B[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001B[0;32m/Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:889\u001B[0m, in \u001B[0;36mTrainer.setup\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m    886\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menv\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_env_id\n\u001B[1;32m    888\u001B[0m \u001B[38;5;66;03m# Validate the framework settings in config.\u001B[39;00m\n\u001B[0;32m--> 889\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_framework\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    891\u001B[0m \u001B[38;5;66;03m# Setup the self.env_creator callable (to be passed\u001B[39;00m\n\u001B[1;32m    892\u001B[0m \u001B[38;5;66;03m# e.g. to RolloutWorkers' c'tors).\u001B[39;00m\n\u001B[1;32m    893\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv_creator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_env_creator_from_env_id(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_env_id)\n",
      "File \u001B[0;32m/Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:2340\u001B[0m, in \u001B[0;36mTrainer.validate_framework\u001B[0;34m(config)\u001B[0m\n\u001B[1;32m   2331\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m _tf1 \u001B[38;5;129;01mand\u001B[39;00m config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mframework\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   2332\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[1;32m   2333\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour framework setting is \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, meaning you are using \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2334\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatic-graph mode. Set framework=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtf2\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to enable eager \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2337\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspeed as with static-graph mode.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2338\u001B[0m         )\n\u001B[0;32m-> 2340\u001B[0m \u001B[43mcheck_if_correct_nn_framework_installed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2341\u001B[0m resolve_tf_settings()\n",
      "File \u001B[0;32m/Applications/anaconda3/envs/ExploringRL/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:2299\u001B[0m, in \u001B[0;36mTrainer.validate_framework.<locals>.check_if_correct_nn_framework_installed\u001B[0;34m()\u001B[0m\n\u001B[1;32m   2297\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m framework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   2298\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _torch:\n\u001B[0;32m-> 2299\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m   2300\u001B[0m             (\n\u001B[1;32m   2301\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPyTorch was specified as the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mframework\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m inside \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2302\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mof your config dictionary. However, there was no \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2303\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstallation found. You can install PyTorch via \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2304\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`pip install torch`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2305\u001B[0m             )\n\u001B[1;32m   2306\u001B[0m         )\n",
      "\u001B[0;31mImportError\u001B[0m: PyTorch was specified as the 'framework' inside of your config dictionary. However, there was no installation found. You can install PyTorch via `pip install torch`"
     ]
    }
   ],
   "source": [
    "# Configure the algorithm.\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"Taxi-v3\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 2,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"torch\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create our RLlib Trainer.\n",
    "trainer = PPOTrainer(config=config)\n",
    "\n",
    "# Run it for n training iterations. A training iteration includes\n",
    "# parallel sample collection by the environment workers as well as\n",
    "# loss calculation on the collected batch and a model update.\n",
    "for _ in range(3):\n",
    "    print(trainer.train())\n",
    "\n",
    "# Evaluate the trained Trainer (and render each timestep to the shell's\n",
    "# output).\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}