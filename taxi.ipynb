{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Taxi Driver \n",
    "One of the tutorials recommended on open-ai gym docs. Instructions found here https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radom walk to taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001B[35m\u001B[34;1m\u001B[43mR\u001B[0m\u001B[0m\u001B[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 3191\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n",
      "Timesteps taken: 3191\n",
      "Penalties incurred: 1023\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.s = 328  # set environment to illustration's state\n",
    "env.render()\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = []  # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "\n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "    }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "\n",
    "\n",
    "print_frames(frames)\n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 47 s, sys: 8.62 s, total: 55.6 s\n",
      "Wall time: 46.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.40416292,  -2.27325184,  -2.39922046,  -2.35773077,\n",
       "       -10.52651192, -10.21776502])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.41\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the algorithm.\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"Taxi-v3\",\n",
    "    \"render_env\": \"~/Documents/ExploringRL/Recordings\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 4,\n",
    "    \"horizon\": 10000,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"torch\",\n",
    "    # Since learning is most of the time done on the local worker,\n",
    "    # it may help to provide one or more GPUs to that worker via the num_gpus setting\n",
    "    \"num_gpus\": 1,\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create our RLlib Trainer.\n",
    "trainer = PPOTrainer(config=config)\n",
    "\n",
    "# Run it for n training iterations. A training iteration includes\n",
    "# parallel sample collection by the environment workers as well as\n",
    "# loss calculation on the collected batch and a model update.\n",
    "for _ in range(10):\n",
    "    print(trainer.train())\n",
    "\n",
    "# Evaluate the trained Trainer (and render each timestep to the shell's\n",
    "# output).\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"trainer.pkl\", 'wb') as f:\n",
    "    pickle.dump(agent, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate PPO agent's performance after learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = trainer.compute_single_action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env\n",
    "print(env.s)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.logger import DEFAULT_LOGGERS\n",
    "from ray.tune.integration.wandb import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_root = \"tmp/ppo/taxi\"\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)   # clean up old runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`callbacks` must be a callable method that returns a subclass of DefaultCallbacks, got <ray.tune.integration.wandb.WandbLoggerCallback object at 0x7fc5c6d16d30>!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      7\u001B[0m config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_gpus\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m\n\u001B[1;32m      8\u001B[0m config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m WandbLoggerCallback(api_key_file\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/home/sem22h2/.netrc\u001B[39m\u001B[38;5;124m\"\u001B[39m, project\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtaxi-v3\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m agent \u001B[38;5;241m=\u001B[39m \u001B[43mppo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPPOTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mSELECT_ENV\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m results \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     12\u001B[0m episode_data \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/.conda/envs/RL/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:870\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001B[0m\n\u001B[1;32m    858\u001B[0m \u001B[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001B[39;00m\n\u001B[1;32m    859\u001B[0m \u001B[38;5;66;03m# available. We want to make sure the metrics are always present\u001B[39;00m\n\u001B[1;32m    860\u001B[0m \u001B[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001B[39;00m\n\u001B[1;32m    861\u001B[0m \u001B[38;5;66;03m# when we use these as stopping criteria.\u001B[39;00m\n\u001B[1;32m    862\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation_metrics \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    863\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m    864\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepisode_reward_max\u001B[39m\u001B[38;5;124m\"\u001B[39m: np\u001B[38;5;241m.\u001B[39mnan,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    867\u001B[0m     }\n\u001B[1;32m    868\u001B[0m }\n\u001B[0;32m--> 870\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    871\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogger_creator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremote_checkpoint_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msync_function_tpl\u001B[49m\n\u001B[1;32m    872\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/RL/lib/python3.9/site-packages/ray/tune/trainable.py:156\u001B[0m, in \u001B[0;36mTrainable.__init__\u001B[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001B[0m\n\u001B[1;32m    154\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_local_ip \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_current_ip()\n\u001B[0;32m--> 156\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeepcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m setup_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m setup_time \u001B[38;5;241m>\u001B[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001B[0;32m~/.conda/envs/RL/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:899\u001B[0m, in \u001B[0;36mTrainer.setup\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m    895\u001B[0m \u001B[38;5;66;03m# Set Trainer's seed after we have - if necessary - enabled\u001B[39;00m\n\u001B[1;32m    896\u001B[0m \u001B[38;5;66;03m# tf eager-execution.\u001B[39;00m\n\u001B[1;32m    897\u001B[0m update_global_seed_if_necessary(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mframework\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m--> 899\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_config\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    900\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m]()\n\u001B[1;32m    901\u001B[0m log_level \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlog_level\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/RL/lib/python3.9/site-packages/ray/rllib/agents/ppo/ppo.py:344\u001B[0m, in \u001B[0;36mPPOTrainer.validate_config\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m    335\u001B[0m \u001B[38;5;124;03m\"\"\"Validates the Trainer's config dict.\u001B[39;00m\n\u001B[1;32m    336\u001B[0m \n\u001B[1;32m    337\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    341\u001B[0m \u001B[38;5;124;03m    ValueError: In case something is wrong with the config.\u001B[39;00m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;66;03m# Call super's validation method.\u001B[39;00m\n\u001B[0;32m--> 344\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mentropy_coeff\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;28mint\u001B[39m):\n\u001B[1;32m    347\u001B[0m     config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mentropy_coeff\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mentropy_coeff\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[0;32m~/.conda/envs/RL/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:2376\u001B[0m, in \u001B[0;36mTrainer.validate_config\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m   2374\u001B[0m \u001B[38;5;66;03m# Check, whether given `callbacks` is a callable.\u001B[39;00m\n\u001B[1;32m   2375\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m callable(config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m]):\n\u001B[0;32m-> 2376\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2377\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`callbacks` must be a callable method that \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2378\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturns a subclass of DefaultCallbacks, got \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2379\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2380\u001B[0m     )\n\u001B[1;32m   2382\u001B[0m \u001B[38;5;66;03m# Multi-GPU settings.\u001B[39;00m\n\u001B[1;32m   2383\u001B[0m simple_optim_setting \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimple_optimizer\u001B[39m\u001B[38;5;124m\"\u001B[39m, DEPRECATED_VALUE)\n",
      "\u001B[0;31mValueError\u001B[0m: `callbacks` must be a callable method that returns a subclass of DefaultCallbacks, got <ray.tune.integration.wandb.WandbLoggerCallback object at 0x7fc5c6d16d30>!"
     ]
    }
   ],
   "source": [
    "SELECT_ENV = \"Taxi-v3\"\n",
    "N_ITER = 20\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "config[\"framework\"] = \"torch\"\n",
    "config[\"num_gpus\"] = 4\n",
    "# config[\"callbacks\"] = WandbLoggerCallback(api_key_file=\"/home/sem22h2/.netrc\", project=\"taxi-v3\")\n",
    "\n",
    "agent = ppo.PPOTrainer(config, env=SELECT_ENV)\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min': result['episode_reward_min'], \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']\n",
    "              }\n",
    "    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    \n",
    "    print(f'{n+1:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}, len mean: {result[\"episode_len_mean\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0484,  0.0386, -0.0780,  ..., -0.0539, -0.0186, -0.0189],\n",
      "        [-0.0605, -0.0826, -0.0100,  ..., -0.0543, -0.0494, -0.0333],\n",
      "        [ 0.0751,  0.0094,  0.0455,  ...,  0.1059,  0.0092, -0.0261],\n",
      "        [-0.0117, -0.0598,  0.0818,  ...,  0.0364, -0.0492, -0.0255],\n",
      "        [-0.0592,  0.0431, -0.0732,  ..., -0.0676,  0.0676,  0.0304],\n",
      "        [ 0.0083,  0.0732,  0.0018,  ...,  0.0290,  0.0547,  0.0975]],\n",
      "       device='cuda:0', requires_grad=True),\n",
      " Parameter containing:\n",
      "tensor([ 0.0043,  0.0148,  0.0079,  0.0059, -0.0152, -0.0172], device='cuda:0',\n",
      "       requires_grad=True),\n",
      " Parameter containing:\n",
      "tensor([[ 0.0708,  0.0101, -0.0280,  ..., -0.0074, -0.0139, -0.0591],\n",
      "        [ 0.0276,  0.0846, -0.0561,  ...,  0.0012, -0.0517,  0.1160],\n",
      "        [-0.0172,  0.0208, -0.0893,  ..., -0.0526, -0.0121, -0.0697],\n",
      "        ...,\n",
      "        [-0.0116, -0.0449, -0.0796,  ..., -0.0394,  0.0614,  0.0321],\n",
      "        [ 0.0129, -0.0071,  0.0489,  ...,  0.0340, -0.0889,  0.0209],\n",
      "        [ 0.0300,  0.0055, -0.0681,  ..., -0.0423, -0.0265,  0.0378]],\n",
      "       device='cuda:0', requires_grad=True),\n",
      " Parameter containing:\n",
      "tensor([-0.0091,  0.0195, -0.0180, -0.0044,  0.0071, -0.0127,  0.0030, -0.0201,\n",
      "        -0.0078, -0.0245, -0.0213,  0.0226,  0.0254, -0.0103, -0.0230,  0.0202,\n",
      "        -0.0163,  0.0057, -0.0086, -0.0078,  0.0171, -0.0238, -0.0073,  0.0045,\n",
      "        -0.0073, -0.0176,  0.0230,  0.0018,  0.0188, -0.0004,  0.0018, -0.0229,\n",
      "        -0.0124,  0.0117,  0.0226, -0.0174, -0.0115, -0.0078,  0.0213,  0.0196,\n",
      "        -0.0087, -0.0121, -0.0238,  0.0038, -0.0171,  0.0193,  0.0023,  0.0179,\n",
      "        -0.0140,  0.0196,  0.0091, -0.0145,  0.0217, -0.0243, -0.0124,  0.0159,\n",
      "         0.0136,  0.0170, -0.0188, -0.0217,  0.0192,  0.0154,  0.0179,  0.0030,\n",
      "         0.0244, -0.0181,  0.0141,  0.0191, -0.0119,  0.0164, -0.0066,  0.0230,\n",
      "        -0.0191, -0.0240,  0.0138,  0.0021, -0.0080,  0.0042,  0.0075, -0.0011,\n",
      "        -0.0059, -0.0057, -0.0110,  0.0076,  0.0060, -0.0051, -0.0036, -0.0076,\n",
      "        -0.0204,  0.0037,  0.0078,  0.0210,  0.0051,  0.0150,  0.0111, -0.0089,\n",
      "        -0.0079, -0.0055,  0.0176, -0.0112,  0.0072,  0.0129,  0.0031, -0.0035,\n",
      "         0.0210, -0.0151, -0.0157,  0.0019, -0.0149,  0.0014,  0.0132,  0.0033,\n",
      "        -0.0115,  0.0027,  0.0003,  0.0112, -0.0163,  0.0147,  0.0041, -0.0117,\n",
      "         0.0140,  0.0141,  0.0018, -0.0135, -0.0124, -0.0142, -0.0170,  0.0221,\n",
      "        -0.0035, -0.0217,  0.0048, -0.0154, -0.0126,  0.0188, -0.0165,  0.0088,\n",
      "         0.0173, -0.0175,  0.0129, -0.0201,  0.0145, -0.0002,  0.0007,  0.0194,\n",
      "         0.0123, -0.0253, -0.0112,  0.0225, -0.0048, -0.0129,  0.0158,  0.0247,\n",
      "        -0.0036,  0.0144, -0.0253, -0.0283, -0.0128, -0.0169,  0.0047, -0.0086,\n",
      "         0.0195, -0.0035, -0.0018,  0.0070,  0.0204, -0.0200, -0.0224, -0.0117,\n",
      "         0.0184,  0.0060, -0.0001, -0.0108, -0.0084, -0.0022,  0.0105,  0.0024,\n",
      "        -0.0069,  0.0157,  0.0248, -0.0212,  0.0137,  0.0055, -0.0058, -0.0181,\n",
      "         0.0059,  0.0118,  0.0050,  0.0188,  0.0105, -0.0185, -0.0208,  0.0075,\n",
      "        -0.0028,  0.0175,  0.0194, -0.0104, -0.0055, -0.0177, -0.0038,  0.0083,\n",
      "        -0.0161,  0.0093,  0.0002,  0.0103, -0.0127,  0.0009, -0.0154,  0.0090,\n",
      "         0.0171,  0.0109, -0.0089, -0.0204, -0.0148, -0.0064,  0.0085, -0.0009,\n",
      "        -0.0145,  0.0186, -0.0186,  0.0017,  0.0306, -0.0174,  0.0112, -0.0044,\n",
      "        -0.0123,  0.0196,  0.0012, -0.0027,  0.0236, -0.0029,  0.0221, -0.0024,\n",
      "         0.0119,  0.0281,  0.0087,  0.0263, -0.0131,  0.0057,  0.0102, -0.0035,\n",
      "         0.0215, -0.0014, -0.0084,  0.0005,  0.0086,  0.0167, -0.0212, -0.0080,\n",
      "         0.0196,  0.0253, -0.0142, -0.0158, -0.0028,  0.0065,  0.0144, -0.0246],\n",
      "       device='cuda:0', requires_grad=True),\n",
      " Parameter containing:\n",
      "tensor([[ 0.0449,  0.0145, -0.0349,  ...,  0.0384,  0.1236, -0.0132],\n",
      "        [ 0.0972, -0.2094, -0.0433,  ..., -0.0052,  0.1167,  0.1347],\n",
      "        [ 0.0291,  0.1104,  0.0425,  ...,  0.0109, -0.0303, -0.0067],\n",
      "        ...,\n",
      "        [-0.0054,  0.1134,  0.0204,  ..., -0.0208, -0.0358,  0.0300],\n",
      "        [-0.0922, -0.1721,  0.0632,  ..., -0.0886,  0.0434,  0.0260],\n",
      "        [-0.0502,  0.0316,  0.0319,  ..., -0.1571, -0.0100, -0.0088]],\n",
      "       device='cuda:0', requires_grad=True),\n",
      " Parameter containing:\n",
      "tensor([ 8.4984e-03, -2.5387e-02,  1.4151e-02, -1.1998e-02,  1.6228e-02,\n",
      "        -3.0415e-03,  7.4499e-03,  5.5117e-03, -1.8603e-02,  1.1522e-02,\n",
      "        -1.4476e-02,  1.5791e-02, -1.1877e-02,  4.4759e-03,  9.4727e-03,\n",
      "         2.3702e-03,  1.0108e-02, -1.4229e-02, -1.6221e-02, -1.4817e-02,\n",
      "        -8.8206e-03, -1.5487e-02, -1.3814e-02,  5.8767e-03, -2.2573e-02,\n",
      "        -5.4787e-03, -2.2387e-02, -1.3140e-02, -1.3731e-03, -1.5388e-02,\n",
      "        -1.5313e-02,  1.6431e-02, -5.2535e-03,  2.3174e-02,  2.2934e-02,\n",
      "        -2.4264e-02,  1.7690e-02, -7.4735e-04, -5.3699e-03, -3.5523e-03,\n",
      "         2.7401e-02,  1.3409e-02,  6.9527e-03,  8.1532e-03, -1.1922e-02,\n",
      "        -2.3728e-02,  2.2541e-02,  1.1625e-03, -2.3707e-02, -9.7408e-03,\n",
      "        -1.0650e-03,  7.3081e-03,  2.4880e-02, -5.1213e-04,  1.6561e-02,\n",
      "        -2.0295e-02,  1.1769e-02,  4.7009e-03,  1.3647e-03, -2.4572e-02,\n",
      "        -7.1456e-04, -5.8479e-03,  2.1888e-02,  2.4723e-02, -1.3824e-02,\n",
      "        -2.4561e-02,  9.5180e-03, -6.5066e-03,  1.1454e-02,  2.6600e-03,\n",
      "        -1.3215e-02,  2.2147e-02,  8.3710e-03,  1.8196e-02, -1.8088e-02,\n",
      "        -9.3527e-03, -5.0664e-03,  2.1837e-02, -1.2665e-02, -9.6632e-03,\n",
      "         1.6344e-02,  1.8939e-02,  1.0270e-02, -2.1530e-02, -1.3149e-02,\n",
      "        -1.8307e-02,  1.1565e-02,  2.0153e-02,  1.7038e-02, -1.8858e-02,\n",
      "         1.0103e-02, -1.8638e-02, -2.9765e-05, -1.0148e-02,  4.8497e-03,\n",
      "        -6.3309e-03,  1.1804e-02, -1.1621e-03, -1.7690e-02,  2.2627e-02,\n",
      "        -3.7211e-03,  2.2171e-02, -4.2373e-03,  1.4434e-02, -9.4148e-03,\n",
      "         1.6993e-02,  1.3789e-02, -3.1747e-03, -1.1244e-02,  1.6571e-02,\n",
      "        -1.3314e-02, -1.2157e-02,  5.2864e-03,  1.2813e-02,  2.0583e-02,\n",
      "        -1.9433e-02, -1.9077e-02, -3.4120e-03,  9.4830e-03,  7.8659e-03,\n",
      "        -6.9625e-03, -1.9103e-02, -7.0208e-03,  8.3948e-03,  1.3411e-02,\n",
      "        -2.1777e-02, -5.2906e-03, -7.0426e-03, -1.2439e-02, -2.0286e-02,\n",
      "        -8.4457e-03, -7.6659e-03, -2.6424e-02,  1.2153e-02, -1.3286e-02,\n",
      "        -5.7671e-04, -6.6265e-03,  2.3197e-02, -1.8827e-03, -4.4562e-03,\n",
      "         1.6360e-02, -1.1571e-02, -6.4800e-03,  1.0351e-02,  8.1860e-04,\n",
      "        -1.8985e-03, -6.2204e-03,  1.7174e-02,  7.7569e-04, -1.2306e-02,\n",
      "         2.4026e-02,  1.8551e-02, -1.1175e-02, -8.4202e-03, -8.9129e-04,\n",
      "        -9.8800e-03,  1.2052e-02, -8.5064e-03, -7.5453e-03,  3.2389e-03,\n",
      "         7.8195e-03, -2.2424e-02,  1.5730e-03, -1.1627e-02, -7.9483e-03,\n",
      "        -1.0737e-02,  9.4791e-04, -1.3817e-02,  1.1365e-02, -2.1815e-02,\n",
      "         1.1456e-03, -2.2101e-03, -1.3153e-02,  8.4297e-03,  2.6125e-02,\n",
      "         1.8430e-02,  1.5399e-02,  4.2904e-03,  2.7519e-03,  8.8656e-03,\n",
      "        -1.5795e-02, -6.4776e-03,  5.1833e-03, -9.0351e-03,  3.0645e-03,\n",
      "         4.6647e-03,  9.8870e-04, -2.9245e-02, -4.5009e-03, -1.7999e-02,\n",
      "        -1.5579e-02, -2.5826e-02, -1.7922e-02,  2.0881e-02, -5.1737e-03,\n",
      "        -1.6850e-02, -2.6709e-02,  1.3228e-02,  1.3376e-02, -3.6107e-03,\n",
      "        -1.2923e-02,  1.6695e-02,  1.4731e-02,  2.1726e-02,  1.8730e-02,\n",
      "         2.9071e-03,  1.6853e-02, -1.3363e-02, -1.0976e-02,  2.3261e-02,\n",
      "         2.4781e-02, -1.3467e-02,  1.5611e-03,  8.9757e-03, -2.5802e-02,\n",
      "        -2.3292e-02, -2.3251e-02,  8.8208e-03,  9.6913e-03, -9.7634e-03,\n",
      "        -2.1613e-02,  1.6542e-02,  1.8869e-02,  2.0095e-02, -1.7957e-02,\n",
      "        -3.4526e-04,  5.9621e-03,  1.4370e-03,  1.4366e-05,  6.0533e-03,\n",
      "        -1.8846e-02, -1.4588e-02, -5.1784e-04,  1.2469e-02, -1.0121e-02,\n",
      "        -1.9093e-02,  4.1361e-03,  2.2803e-02, -1.8903e-02,  9.4392e-03,\n",
      "        -1.9119e-02, -2.6751e-02, -1.0389e-02,  8.2352e-03,  1.3801e-02,\n",
      "        -6.3082e-03, -8.1410e-03, -6.7313e-03,  2.2970e-02, -1.3159e-02,\n",
      "        -1.7085e-03,  2.0107e-03,  1.4986e-02,  3.3344e-03, -2.6964e-02,\n",
      "        -2.4938e-02], device='cuda:0', requires_grad=True),\n",
      " Parameter containing:\n",
      "tensor([[-0.1107, -0.0352,  0.0265,  ..., -0.0105, -0.0161, -0.0340],\n",
      "        [ 0.0561, -0.0277,  0.0233,  ..., -0.0042, -0.0320, -0.1511],\n",
      "        [-0.0118, -0.1032,  0.0656,  ..., -0.0412,  0.0269,  0.0893],\n",
      "        ...,\n",
      "        [-0.0818,  0.0849,  0.0538,  ..., -0.0303, -0.0242, -0.1017],\n",
      "        [-0.0480,  0.0219, -0.1011,  ..., -0.0355, -0.0020, -0.0157],\n",
      "        [-0.0644, -0.0036,  0.0089,  ..., -0.0511, -0.0510, -0.1216]],\n",
      "       device='cuda:0', requires_grad=True),\n",
      " Parameter containing:\n",
      "tensor([ 0.0340,  0.0349, -0.0371, -0.0339,  0.0343,  0.0367,  0.0334, -0.0332,\n",
      "         0.0347,  0.0337, -0.0353,  0.0330,  0.0337,  0.0336, -0.0333,  0.0400,\n",
      "         0.0355, -0.0342, -0.0345, -0.0320,  0.0348, -0.0371, -0.0342, -0.0339,\n",
      "         0.0329, -0.0353,  0.0302, -0.0362, -0.0352,  0.0344, -0.0306, -0.0373,\n",
      "        -0.0346, -0.0369,  0.0357,  0.0372,  0.0350, -0.0331,  0.0195,  0.0359,\n",
      "        -0.0343,  0.0366,  0.0353, -0.0354,  0.0353, -0.0349, -0.0335,  0.0350,\n",
      "         0.0353, -0.0344, -0.0347,  0.0349, -0.0354,  0.0353,  0.0355, -0.0333,\n",
      "         0.0358, -0.0356, -0.0336,  0.0348, -0.0346, -0.0359,  0.0358,  0.0337,\n",
      "        -0.0353, -0.0349, -0.0354,  0.0338,  0.0347,  0.0354, -0.0372,  0.0340,\n",
      "        -0.0349,  0.0336, -0.0362, -0.0358,  0.0360, -0.0377,  0.0355,  0.0363,\n",
      "         0.0355, -0.0363,  0.0292, -0.0381, -0.0336, -0.0333, -0.0345, -0.0344,\n",
      "        -0.0348,  0.0348, -0.0322,  0.0285,  0.0337,  0.0357,  0.0339,  0.0349,\n",
      "        -0.0341,  0.0364,  0.0355, -0.0365, -0.0399,  0.0368,  0.0335, -0.0356,\n",
      "         0.0353, -0.0309,  0.0352,  0.0358, -0.0329, -0.0322,  0.0334,  0.0369,\n",
      "         0.0357,  0.0352,  0.0359,  0.0346,  0.0360,  0.0350,  0.0351, -0.0367,\n",
      "        -0.0351,  0.0341,  0.0344,  0.0334,  0.0335, -0.0316,  0.0344, -0.0328,\n",
      "        -0.0361, -0.0337, -0.0370,  0.0336,  0.0383, -0.0367, -0.0385,  0.0323,\n",
      "        -0.0347, -0.0295,  0.0348,  0.0336,  0.0339,  0.0340, -0.0366,  0.0353,\n",
      "        -0.0346,  0.0367, -0.0343,  0.0379,  0.0346, -0.0321,  0.0362,  0.0328,\n",
      "         0.0337,  0.0354,  0.0354, -0.0363,  0.0347, -0.0350, -0.0334,  0.0340,\n",
      "        -0.0368,  0.0342, -0.0372, -0.0341,  0.0355, -0.0332, -0.0312,  0.0348,\n",
      "         0.0350, -0.0364, -0.0345, -0.0352, -0.0365, -0.0350, -0.0339,  0.0334,\n",
      "         0.0344,  0.0337,  0.0337, -0.0274, -0.0361,  0.0319, -0.0355,  0.0312,\n",
      "         0.0334, -0.0357,  0.0361,  0.0340,  0.0329, -0.0327, -0.0362,  0.0334,\n",
      "        -0.0329, -0.0358,  0.0336,  0.0364, -0.0341, -0.0347,  0.0344,  0.0362,\n",
      "         0.0346, -0.0352, -0.0364,  0.0337,  0.0349,  0.0350, -0.0328, -0.0346,\n",
      "        -0.0359, -0.0331,  0.0351, -0.0355, -0.0359, -0.0366,  0.0311,  0.0344,\n",
      "        -0.0367,  0.0352, -0.0333, -0.0364, -0.0359, -0.0351, -0.0286, -0.0338,\n",
      "         0.0354, -0.0360,  0.0325, -0.0372, -0.0347, -0.0349,  0.0331,  0.0346,\n",
      "        -0.0350, -0.0350,  0.0346,  0.0335,  0.0344,  0.0344, -0.0358, -0.0330,\n",
      "         0.0355, -0.0326,  0.0346, -0.0338, -0.0350,  0.0350,  0.0363, -0.0354,\n",
      "         0.0343,  0.0354,  0.0349, -0.0307,  0.0363,  0.0347,  0.0351,  0.0333],\n",
      "       device='cuda:0', requires_grad=True),\n",
      " Parameter containing:\n",
      "tensor([[-0.0542, -0.0377,  0.0477,  ..., -0.0159, -0.0139, -0.1104],\n",
      "        [ 0.1784,  0.1107, -0.1318,  ...,  0.0072,  0.0486,  0.0583],\n",
      "        [-0.0919,  0.0631, -0.0320,  ..., -0.0154,  0.0620, -0.1632],\n",
      "        ...,\n",
      "        [ 0.1872,  0.0414, -0.0117,  ...,  0.1148, -0.0636,  0.1144],\n",
      "        [-0.1146, -0.1213,  0.0390,  ..., -0.0553,  0.0564, -0.1235],\n",
      "        [ 0.1659,  0.0607, -0.0732,  ..., -0.1363,  0.0371,  0.0619]],\n",
      "       device='cuda:0', requires_grad=True),\n",
      " Parameter containing:\n",
      "tensor([-0.0349,  0.0267, -0.0317,  0.0281, -0.0257, -0.0280, -0.0285,  0.0316,\n",
      "        -0.0288, -0.0276,  0.0290, -0.0276, -0.0316,  0.0283,  0.0265,  0.0252,\n",
      "        -0.0338,  0.0314,  0.0316,  0.0289,  0.0280, -0.0257,  0.0310, -0.0313,\n",
      "        -0.0284,  0.0292, -0.0263,  0.0291,  0.0310,  0.0307,  0.0292,  0.0317,\n",
      "         0.0294,  0.0309, -0.0310,  0.0339,  0.0285,  0.0292,  0.0306,  0.0278,\n",
      "        -0.0284, -0.0336,  0.0307, -0.0271, -0.0261, -0.0278, -0.0254, -0.0317,\n",
      "        -0.0334, -0.0287,  0.0307, -0.0241, -0.0290,  0.0349, -0.0312,  0.0344,\n",
      "         0.0320, -0.0328,  0.0286,  0.0304, -0.0311, -0.0302,  0.0341,  0.0296,\n",
      "        -0.0308,  0.0351,  0.0314,  0.0232, -0.0299, -0.0322, -0.0306, -0.0287,\n",
      "        -0.0317,  0.0257, -0.0281, -0.0269,  0.0330, -0.0234, -0.0281, -0.0268,\n",
      "        -0.0287, -0.0315, -0.0338,  0.0298, -0.0302,  0.0312, -0.0301, -0.0327,\n",
      "         0.0256,  0.0294,  0.0296,  0.0322, -0.0267, -0.0276,  0.0325,  0.0297,\n",
      "         0.0262,  0.0328, -0.0318,  0.0293, -0.0334,  0.0335,  0.0295,  0.0318,\n",
      "        -0.0247,  0.0295,  0.0302,  0.0264,  0.0352, -0.0280, -0.0323,  0.0272,\n",
      "         0.0326,  0.0309,  0.0289, -0.0327,  0.0295,  0.0320, -0.0311, -0.0306,\n",
      "        -0.0316, -0.0306,  0.0258,  0.0297, -0.0259, -0.0278, -0.0283,  0.0241,\n",
      "         0.0301,  0.0303,  0.0319, -0.0319,  0.0300,  0.0279, -0.0248, -0.0305,\n",
      "        -0.0298, -0.0281, -0.0268,  0.0244, -0.0344, -0.0311, -0.0239,  0.0299,\n",
      "        -0.0285, -0.0280,  0.0330,  0.0290, -0.0290, -0.0303, -0.0318, -0.0276,\n",
      "         0.0341,  0.0281, -0.0271,  0.0323,  0.0318,  0.0310, -0.0263,  0.0300,\n",
      "         0.0327,  0.0265, -0.0287, -0.0268, -0.0277,  0.0336,  0.0295,  0.0267,\n",
      "         0.0313, -0.0303,  0.0345, -0.0242, -0.0311,  0.0320, -0.0332, -0.0297,\n",
      "         0.0300, -0.0353, -0.0291, -0.0303,  0.0246, -0.0334, -0.0277,  0.0254,\n",
      "        -0.0324, -0.0274, -0.0284,  0.0308,  0.0300, -0.0301, -0.0264, -0.0292,\n",
      "         0.0318,  0.0325,  0.0302, -0.0261, -0.0317, -0.0313,  0.0283, -0.0307,\n",
      "         0.0323, -0.0335,  0.0262, -0.0310,  0.0318, -0.0303,  0.0289,  0.0303,\n",
      "         0.0317, -0.0301,  0.0292,  0.0301, -0.0316, -0.0326, -0.0282, -0.0326,\n",
      "        -0.0292,  0.0322, -0.0336, -0.0254,  0.0342,  0.0332, -0.0317,  0.0303,\n",
      "        -0.0301, -0.0302,  0.0304, -0.0274, -0.0313, -0.0284, -0.0305,  0.0301,\n",
      "        -0.0319,  0.0345, -0.0327, -0.0298, -0.0331, -0.0300,  0.0300, -0.0301,\n",
      "         0.0306, -0.0307,  0.0347,  0.0307, -0.0286, -0.0311, -0.0301, -0.0319,\n",
      "        -0.0272, -0.0318,  0.0353,  0.0313,  0.0301,  0.0312, -0.0296,  0.0283],\n",
      "       device='cuda:0', requires_grad=True),\n",
      " Parameter containing:\n",
      "tensor([[ 0.0799, -0.0830,  0.0806, -0.0845,  0.0810,  0.0755,  0.0798, -0.0830,\n",
      "          0.0824,  0.0822, -0.0807,  0.0843,  0.0779, -0.0874, -0.0815, -0.0843,\n",
      "          0.0855, -0.0816, -0.0796, -0.0852, -0.0837,  0.0838, -0.0832,  0.0834,\n",
      "          0.0841, -0.0883,  0.0800, -0.0814, -0.0835, -0.0794, -0.0809, -0.0833,\n",
      "         -0.0832, -0.0807,  0.0773, -0.0853, -0.0817, -0.0851, -0.0869, -0.0835,\n",
      "          0.0887,  0.0794, -0.0834,  0.0853,  0.0865,  0.0820,  0.0819,  0.0838,\n",
      "          0.0784,  0.0770, -0.0792,  0.0846,  0.0802, -0.0842,  0.0829, -0.0822,\n",
      "         -0.0858,  0.0863, -0.0847, -0.0822,  0.0852,  0.0850, -0.0839, -0.0806,\n",
      "          0.0815, -0.0801, -0.0814, -0.0818,  0.0834,  0.0781,  0.0825,  0.0859,\n",
      "          0.0824, -0.0785,  0.0850,  0.0865, -0.0767,  0.0866,  0.0792,  0.0876,\n",
      "          0.0838,  0.0792,  0.0871, -0.0806,  0.0856, -0.0824,  0.0798,  0.0783,\n",
      "         -0.0814, -0.0816, -0.0830, -0.0850,  0.0839,  0.0846, -0.0833, -0.0795,\n",
      "         -0.0882, -0.0825,  0.0821, -0.0864,  0.0756, -0.0754, -0.0811, -0.0781,\n",
      "          0.0829, -0.0846, -0.0779, -0.0825, -0.0780,  0.0850,  0.0855, -0.0805,\n",
      "         -0.0824, -0.0861, -0.0847,  0.0814, -0.0836, -0.0816,  0.0803,  0.0705,\n",
      "          0.0807,  0.0775, -0.0881, -0.0828,  0.0788,  0.0820,  0.0834, -0.0809,\n",
      "         -0.0809, -0.0816, -0.0810,  0.0788, -0.0830, -0.0843,  0.0830,  0.0815,\n",
      "          0.0851,  0.0867,  0.0806, -0.0826,  0.0843,  0.0862,  0.0835, -0.0794,\n",
      "          0.0799,  0.0816, -0.0825, -0.0892,  0.0798,  0.0816,  0.0830,  0.0814,\n",
      "         -0.0759, -0.0895,  0.0894, -0.0839, -0.0781, -0.0809,  0.0828, -0.0871,\n",
      "         -0.0780, -0.0838,  0.0894,  0.0890,  0.0861, -0.0807, -0.0831, -0.0818,\n",
      "         -0.0775,  0.0822, -0.0819,  0.0814,  0.0808, -0.0808,  0.0819,  0.0829,\n",
      "         -0.0827,  0.0801,  0.0812,  0.0849, -0.0819,  0.0842,  0.0808, -0.0866,\n",
      "          0.0783,  0.0870,  0.0818, -0.0833, -0.0831,  0.0790,  0.0824,  0.0818,\n",
      "         -0.0821, -0.0790, -0.0804,  0.0796,  0.0834,  0.0854, -0.0839,  0.0815,\n",
      "         -0.0810,  0.0794, -0.0838,  0.0853, -0.0835,  0.0856, -0.0837, -0.0818,\n",
      "         -0.0783,  0.0808, -0.0863, -0.0856,  0.0812,  0.0822,  0.0869,  0.0801,\n",
      "          0.0825, -0.0848,  0.0825,  0.0852, -0.0870, -0.0841,  0.0797, -0.0870,\n",
      "          0.0790,  0.0863, -0.0806,  0.0822,  0.0801,  0.0782,  0.0823, -0.0803,\n",
      "          0.0820, -0.0883,  0.0823,  0.0815,  0.0815,  0.0827, -0.0878,  0.0802,\n",
      "         -0.0841,  0.0809, -0.0787, -0.0837,  0.0821,  0.0877,  0.0806,  0.0828,\n",
      "          0.0778,  0.0822, -0.0848, -0.0824, -0.0813, -0.0844,  0.0852, -0.0896]],\n",
      "       device='cuda:0', requires_grad=True),\n",
      " Parameter containing:\n",
      "tensor([-0.0320], device='cuda:0', requires_grad=True)]\n",
      "tensor([-12.7938, -14.1345, -12.7938, -15.5806, -12.4392, -12.8923, -12.7938,\n",
      "        -14.1345, -12.7938, -14.1345, -12.7938, -12.7938, -12.8923, -12.7938,\n",
      "        -12.8923, -12.7938, -14.1345, -12.7938, -12.7938, -14.1345,  -8.1623,\n",
      "         -8.1623,  -8.1623, -14.1345, -11.1902, -12.0486, -12.0486, -13.0971,\n",
      "        -13.0971, -12.0486, -12.0486, -13.0971], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FullyConnectedNetwork' object has no attribute 'base_model'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [21]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m pprint\u001B[38;5;241m.\u001B[39mpprint(model\u001B[38;5;241m.\u001B[39mvariables())\n\u001B[1;32m      7\u001B[0m pprint\u001B[38;5;241m.\u001B[39mpprint(model\u001B[38;5;241m.\u001B[39mvalue_function())\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[38;5;241m.\u001B[39msummary())\n",
      "File \u001B[0;32m~/.conda/envs/RL/lib/python3.9/site-packages/torch/nn/modules/module.py:1207\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1205\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1206\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1207\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1208\u001B[0m     \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'FullyConnectedNetwork' object has no attribute 'base_model'"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "\n",
    "pprint.pprint(model.variables())\n",
    "pprint.pprint(model.value_function())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.tune.integration.wandb import WandbLoggerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mdanky\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sem22h2/Documents/ExploringRL/wandb/run-20220708_075019-d9332_00000</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/danky/Taxi_v3/runs/d9332_00000\" target=\"_blank\">PPO_Taxi-v3_d9332_00000</a></strong> to <a href=\"https://wandb.ai/danky/Taxi_v3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(PPOTrainer pid=39908)\u001B[0m 2022-07-08 07:50:26,739\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001B[2m\u001B[36m(PPOTrainer pid=39908)\u001B[0m 2022-07-08 07:50:26,739\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RolloutWorker pid=40009)\u001B[0m Setting the path for recording to /home/sem22h2/Documents/ExploringRL/logs/PPO/PPO_Taxi-v3_d9332_00000_0_2022-07-08_07-50-19/\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=40010)\u001B[0m Setting the path for recording to /home/sem22h2/Documents/ExploringRL/logs/PPO/PPO_Taxi-v3_d9332_00000_0_2022-07-08_07-50-19/\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=40008)\u001B[0m Setting the path for recording to /home/sem22h2/Documents/ExploringRL/logs/PPO/PPO_Taxi-v3_d9332_00000_0_2022-07-08_07-50-19/\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=40007)\u001B[0m Setting the path for recording to /home/sem22h2/Documents/ExploringRL/logs/PPO/PPO_Taxi-v3_d9332_00000_0_2022-07-08_07-50-19/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:50:34 (running for 00:00:15.85)<br>Memory usage on this node: 11.4/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(PPOTrainer pid=39908)\u001B[0m 2022-07-08 07:50:34,834\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:50:39 (running for 00:00:20.87)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:50:44 (running for 00:00:25.87)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:50:49 (running for 00:00:30.89)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:50:54 (running for 00:00:35.90)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-50-55\n",
      "  done: false\n",
      "  episode_len_mean: 192.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -161.0\n",
      "  episode_reward_mean: -718.25\n",
      "  episode_reward_min: -911.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7863648080056713\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005373008478970527\n",
      "          policy_loss: -0.007753264146947092\n",
      "          total_loss: 9.930198399738599\n",
      "          vf_explained_var: -0.00038039838114092427\n",
      "          vf_loss: 9.936877073267455\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.983333333333333\n",
      "    ram_util_percent: 9.186666666666664\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07297883143315428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1586504749484829\n",
      "    mean_inference_ms: 1.3015626312850361\n",
      "    mean_raw_obs_processing_ms: 0.2125505800847407\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 192.35\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -161.0\n",
      "    episode_reward_mean: -718.25\n",
      "    episode_reward_min: -911.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 47\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      episode_reward:\n",
      "      - -686.0\n",
      "      - -605.0\n",
      "      - -704.0\n",
      "      - -749.0\n",
      "      - -794.0\n",
      "      - -785.0\n",
      "      - -731.0\n",
      "      - -830.0\n",
      "      - -911.0\n",
      "      - -875.0\n",
      "      - -161.0\n",
      "      - -668.0\n",
      "      - -731.0\n",
      "      - -749.0\n",
      "      - -740.0\n",
      "      - -713.0\n",
      "      - -740.0\n",
      "      - -722.0\n",
      "      - -758.0\n",
      "      - -713.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07297883143315428\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1586504749484829\n",
      "      mean_inference_ms: 1.3015626312850361\n",
      "      mean_raw_obs_processing_ms: 0.2125505800847407\n",
      "  time_since_restore: 20.833040237426758\n",
      "  time_this_iter_s: 20.833040237426758\n",
      "  time_total_s: 20.833040237426758\n",
      "  timers:\n",
      "    learn_throughput: 212.028\n",
      "    learn_time_ms: 18865.396\n",
      "    load_throughput: 743605.0\n",
      "    load_time_ms: 5.379\n",
      "    training_iteration_time_ms: 20813.455\n",
      "    update_time_ms: 5.22\n",
      "  timestamp: 1657259455\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:51:00 (running for 00:00:41.79)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          20.833</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -718.25</td><td style=\"text-align: right;\">                -161</td><td style=\"text-align: right;\">                -911</td><td style=\"text-align: right;\">            192.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:51:05 (running for 00:00:46.80)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          20.833</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -718.25</td><td style=\"text-align: right;\">                -161</td><td style=\"text-align: right;\">                -911</td><td style=\"text-align: right;\">            192.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:51:10 (running for 00:00:51.83)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          20.833</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -718.25</td><td style=\"text-align: right;\">                -161</td><td style=\"text-align: right;\">                -911</td><td style=\"text-align: right;\">            192.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-51-12\n",
      "  done: false\n",
      "  episode_len_mean: 195.825\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -161.0\n",
      "  episode_reward_mean: -747.375\n",
      "  episode_reward_min: -992.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 40\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7641091427495403\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011139419508396909\n",
      "          policy_loss: -0.02964115735864447\n",
      "          total_loss: 9.86916392234064\n",
      "          vf_explained_var: -0.00023465028373144007\n",
      "          vf_loss: 9.8965771859692\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 8000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.170833333333333\n",
      "    ram_util_percent: 9.2\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07191572378751723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15500176385891443\n",
      "    mean_inference_ms: 1.2946328927760093\n",
      "    mean_raw_obs_processing_ms: 0.20788049222776905\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 195.825\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -161.0\n",
      "    episode_reward_mean: -747.375\n",
      "    episode_reward_min: -992.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 47\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 186\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      episode_reward:\n",
      "      - -686.0\n",
      "      - -605.0\n",
      "      - -704.0\n",
      "      - -749.0\n",
      "      - -794.0\n",
      "      - -785.0\n",
      "      - -731.0\n",
      "      - -830.0\n",
      "      - -911.0\n",
      "      - -875.0\n",
      "      - -161.0\n",
      "      - -668.0\n",
      "      - -731.0\n",
      "      - -749.0\n",
      "      - -740.0\n",
      "      - -713.0\n",
      "      - -740.0\n",
      "      - -722.0\n",
      "      - -758.0\n",
      "      - -713.0\n",
      "      - -678.0\n",
      "      - -704.0\n",
      "      - -767.0\n",
      "      - -992.0\n",
      "      - -776.0\n",
      "      - -713.0\n",
      "      - -740.0\n",
      "      - -704.0\n",
      "      - -875.0\n",
      "      - -677.0\n",
      "      - -875.0\n",
      "      - -911.0\n",
      "      - -704.0\n",
      "      - -731.0\n",
      "      - -830.0\n",
      "      - -677.0\n",
      "      - -749.0\n",
      "      - -740.0\n",
      "      - -866.0\n",
      "      - -821.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07191572378751723\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.15500176385891443\n",
      "      mean_inference_ms: 1.2946328927760093\n",
      "      mean_raw_obs_processing_ms: 0.20788049222776905\n",
      "  time_since_restore: 37.75891423225403\n",
      "  time_this_iter_s: 16.92587399482727\n",
      "  time_total_s: 37.75891423225403\n",
      "  timers:\n",
      "    learn_throughput: 235.788\n",
      "    learn_time_ms: 16964.426\n",
      "    load_throughput: 855587.536\n",
      "    load_time_ms: 4.675\n",
      "    training_iteration_time_ms: 18864.254\n",
      "    update_time_ms: 5.149\n",
      "  timestamp: 1657259472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:51:17 (running for 00:00:58.74)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         37.7589</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-747.375</td><td style=\"text-align: right;\">                -161</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">           195.825</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:51:22 (running for 00:01:03.79)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         37.7589</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-747.375</td><td style=\"text-align: right;\">                -161</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">           195.825</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:51:27 (running for 00:01:08.80)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         37.7589</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-747.375</td><td style=\"text-align: right;\">                -161</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">           195.825</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:51:32 (running for 00:01:13.82)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         37.7589</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-747.375</td><td style=\"text-align: right;\">                -161</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">           195.825</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-51-33\n",
      "  done: false\n",
      "  episode_len_mean: 197.21666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -161.0\n",
      "  episode_reward_mean: -750.6166666666667\n",
      "  episode_reward_min: -992.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7331176734739735\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012614530745003521\n",
      "          policy_loss: -0.035756791350982524\n",
      "          total_loss: 9.860579955193304\n",
      "          vf_explained_var: 0.0026598346489731983\n",
      "          vf_loss: 9.89381383875365\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.180000000000001\n",
      "    ram_util_percent: 9.199999999999998\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07152919437956422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15331937614497596\n",
      "    mean_inference_ms: 1.2955932711477993\n",
      "    mean_raw_obs_processing_ms: 0.2051454618534472\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 197.21666666666667\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -161.0\n",
      "    episode_reward_mean: -750.6166666666667\n",
      "    episode_reward_min: -992.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 47\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 186\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      episode_reward:\n",
      "      - -686.0\n",
      "      - -605.0\n",
      "      - -704.0\n",
      "      - -749.0\n",
      "      - -794.0\n",
      "      - -785.0\n",
      "      - -731.0\n",
      "      - -830.0\n",
      "      - -911.0\n",
      "      - -875.0\n",
      "      - -161.0\n",
      "      - -668.0\n",
      "      - -731.0\n",
      "      - -749.0\n",
      "      - -740.0\n",
      "      - -713.0\n",
      "      - -740.0\n",
      "      - -722.0\n",
      "      - -758.0\n",
      "      - -713.0\n",
      "      - -678.0\n",
      "      - -704.0\n",
      "      - -767.0\n",
      "      - -992.0\n",
      "      - -776.0\n",
      "      - -713.0\n",
      "      - -740.0\n",
      "      - -704.0\n",
      "      - -875.0\n",
      "      - -677.0\n",
      "      - -875.0\n",
      "      - -911.0\n",
      "      - -704.0\n",
      "      - -731.0\n",
      "      - -830.0\n",
      "      - -677.0\n",
      "      - -749.0\n",
      "      - -740.0\n",
      "      - -866.0\n",
      "      - -821.0\n",
      "      - -632.0\n",
      "      - -740.0\n",
      "      - -803.0\n",
      "      - -695.0\n",
      "      - -821.0\n",
      "      - -767.0\n",
      "      - -776.0\n",
      "      - -857.0\n",
      "      - -722.0\n",
      "      - -767.0\n",
      "      - -812.0\n",
      "      - -677.0\n",
      "      - -794.0\n",
      "      - -758.0\n",
      "      - -659.0\n",
      "      - -848.0\n",
      "      - -740.0\n",
      "      - -875.0\n",
      "      - -677.0\n",
      "      - -722.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07152919437956422\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.15331937614497596\n",
      "      mean_inference_ms: 1.2955932711477993\n",
      "      mean_raw_obs_processing_ms: 0.2051454618534472\n",
      "  time_since_restore: 58.14323091506958\n",
      "  time_this_iter_s: 20.38431668281555\n",
      "  time_total_s: 58.14323091506958\n",
      "  timers:\n",
      "    learn_throughput: 229.049\n",
      "    learn_time_ms: 17463.533\n",
      "    load_throughput: 828245.454\n",
      "    load_time_ms: 4.829\n",
      "    training_iteration_time_ms: 19365.868\n",
      "    update_time_ms: 5.469\n",
      "  timestamp: 1657259493\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:51:38 (running for 00:01:19.20)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         58.1432</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-750.617</td><td style=\"text-align: right;\">                -161</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">           197.217</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:51:43 (running for 00:01:24.24)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         58.1432</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-750.617</td><td style=\"text-align: right;\">                -161</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">           197.217</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:51:48 (running for 00:01:29.24)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         58.1432</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-750.617</td><td style=\"text-align: right;\">                -161</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">           197.217</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-51-51\n",
      "  done: false\n",
      "  episode_len_mean: 195.4125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -161.0\n",
      "  episode_reward_mean: -734.625\n",
      "  episode_reward_min: -992.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 80\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.697235063199074\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015385766949908463\n",
      "          policy_loss: -0.04312300187765911\n",
      "          total_loss: 9.803589567574122\n",
      "          vf_explained_var: 0.0005163910247946298\n",
      "          vf_loss: 9.843635450383669\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 16000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.2192307692307685\n",
      "    ram_util_percent: 9.2\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07126206457233693\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15223642770194754\n",
      "    mean_inference_ms: 1.296567872337183\n",
      "    mean_raw_obs_processing_ms: 0.203796209679083\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 195.4125\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -161.0\n",
      "    episode_reward_mean: -734.625\n",
      "    episode_reward_min: -992.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 47\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 186\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 63\n",
      "      - 200\n",
      "      - 200\n",
      "      - 168\n",
      "      - 200\n",
      "      - 200\n",
      "      - 169\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      episode_reward:\n",
      "      - -686.0\n",
      "      - -605.0\n",
      "      - -704.0\n",
      "      - -749.0\n",
      "      - -794.0\n",
      "      - -785.0\n",
      "      - -731.0\n",
      "      - -830.0\n",
      "      - -911.0\n",
      "      - -875.0\n",
      "      - -161.0\n",
      "      - -668.0\n",
      "      - -731.0\n",
      "      - -749.0\n",
      "      - -740.0\n",
      "      - -713.0\n",
      "      - -740.0\n",
      "      - -722.0\n",
      "      - -758.0\n",
      "      - -713.0\n",
      "      - -678.0\n",
      "      - -704.0\n",
      "      - -767.0\n",
      "      - -992.0\n",
      "      - -776.0\n",
      "      - -713.0\n",
      "      - -740.0\n",
      "      - -704.0\n",
      "      - -875.0\n",
      "      - -677.0\n",
      "      - -875.0\n",
      "      - -911.0\n",
      "      - -704.0\n",
      "      - -731.0\n",
      "      - -830.0\n",
      "      - -677.0\n",
      "      - -749.0\n",
      "      - -740.0\n",
      "      - -866.0\n",
      "      - -821.0\n",
      "      - -632.0\n",
      "      - -740.0\n",
      "      - -803.0\n",
      "      - -695.0\n",
      "      - -821.0\n",
      "      - -767.0\n",
      "      - -776.0\n",
      "      - -857.0\n",
      "      - -722.0\n",
      "      - -767.0\n",
      "      - -812.0\n",
      "      - -677.0\n",
      "      - -794.0\n",
      "      - -758.0\n",
      "      - -659.0\n",
      "      - -848.0\n",
      "      - -740.0\n",
      "      - -875.0\n",
      "      - -677.0\n",
      "      - -722.0\n",
      "      - -731.0\n",
      "      - -758.0\n",
      "      - -740.0\n",
      "      - -749.0\n",
      "      - -857.0\n",
      "      - -731.0\n",
      "      - -686.0\n",
      "      - -695.0\n",
      "      - -641.0\n",
      "      - -186.0\n",
      "      - -722.0\n",
      "      - -686.0\n",
      "      - -426.0\n",
      "      - -695.0\n",
      "      - -839.0\n",
      "      - -616.0\n",
      "      - -740.0\n",
      "      - -866.0\n",
      "      - -749.0\n",
      "      - -620.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07126206457233693\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.15223642770194754\n",
      "      mean_inference_ms: 1.296567872337183\n",
      "      mean_raw_obs_processing_ms: 0.203796209679083\n",
      "  time_since_restore: 76.9113998413086\n",
      "  time_this_iter_s: 18.768168926239014\n",
      "  time_total_s: 76.9113998413086\n",
      "  timers:\n",
      "    learn_throughput: 231.063\n",
      "    learn_time_ms: 17311.302\n",
      "    load_throughput: 763042.945\n",
      "    load_time_ms: 5.242\n",
      "    training_iteration_time_ms: 19212.569\n",
      "    update_time_ms: 5.935\n",
      "  timestamp: 1657259511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:51:57 (running for 00:01:38.08)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         76.9114</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-734.625</td><td style=\"text-align: right;\">                -161</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">           195.412</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:52:02 (running for 00:01:43.09)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         76.9114</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-734.625</td><td style=\"text-align: right;\">                -161</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">           195.412</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:52:07 (running for 00:01:48.11)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         76.9114</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-734.625</td><td style=\"text-align: right;\">                -161</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">           195.412</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-52-12\n",
      "  done: false\n",
      "  episode_len_mean: 192.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -707.4\n",
      "  episode_reward_min: -992.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 103\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.6548978668387218\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017177758069235376\n",
      "          policy_loss: -0.05036356307325825\n",
      "          total_loss: 9.775694449229906\n",
      "          vf_explained_var: -0.005549960046686152\n",
      "          vf_loss: 9.822622440707299\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 20000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.117241379310345\n",
      "    ram_util_percent: 9.2\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.071109897960592\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15156862722396147\n",
      "    mean_inference_ms: 1.2997260123593022\n",
      "    mean_raw_obs_processing_ms: 0.20338857743990923\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 192.24\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1.0\n",
      "    episode_reward_mean: -707.4\n",
      "    episode_reward_min: -992.0\n",
      "    episodes_this_iter: 23\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 47\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 186\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 63\n",
      "      - 200\n",
      "      - 200\n",
      "      - 168\n",
      "      - 200\n",
      "      - 200\n",
      "      - 169\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 11\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 92\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 88\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      episode_reward:\n",
      "      - -749.0\n",
      "      - -794.0\n",
      "      - -785.0\n",
      "      - -731.0\n",
      "      - -830.0\n",
      "      - -911.0\n",
      "      - -875.0\n",
      "      - -161.0\n",
      "      - -668.0\n",
      "      - -731.0\n",
      "      - -749.0\n",
      "      - -740.0\n",
      "      - -713.0\n",
      "      - -740.0\n",
      "      - -722.0\n",
      "      - -758.0\n",
      "      - -713.0\n",
      "      - -678.0\n",
      "      - -704.0\n",
      "      - -767.0\n",
      "      - -992.0\n",
      "      - -776.0\n",
      "      - -713.0\n",
      "      - -740.0\n",
      "      - -704.0\n",
      "      - -875.0\n",
      "      - -677.0\n",
      "      - -875.0\n",
      "      - -911.0\n",
      "      - -704.0\n",
      "      - -731.0\n",
      "      - -830.0\n",
      "      - -677.0\n",
      "      - -749.0\n",
      "      - -740.0\n",
      "      - -866.0\n",
      "      - -821.0\n",
      "      - -632.0\n",
      "      - -740.0\n",
      "      - -803.0\n",
      "      - -695.0\n",
      "      - -821.0\n",
      "      - -767.0\n",
      "      - -776.0\n",
      "      - -857.0\n",
      "      - -722.0\n",
      "      - -767.0\n",
      "      - -812.0\n",
      "      - -677.0\n",
      "      - -794.0\n",
      "      - -758.0\n",
      "      - -659.0\n",
      "      - -848.0\n",
      "      - -740.0\n",
      "      - -875.0\n",
      "      - -677.0\n",
      "      - -722.0\n",
      "      - -731.0\n",
      "      - -758.0\n",
      "      - -740.0\n",
      "      - -749.0\n",
      "      - -857.0\n",
      "      - -731.0\n",
      "      - -686.0\n",
      "      - -695.0\n",
      "      - -641.0\n",
      "      - -186.0\n",
      "      - -722.0\n",
      "      - -686.0\n",
      "      - -426.0\n",
      "      - -695.0\n",
      "      - -839.0\n",
      "      - -616.0\n",
      "      - -740.0\n",
      "      - -866.0\n",
      "      - -749.0\n",
      "      - -620.0\n",
      "      - -740.0\n",
      "      - -722.0\n",
      "      - -776.0\n",
      "      - -524.0\n",
      "      - -749.0\n",
      "      - 1.0\n",
      "      - -740.0\n",
      "      - -731.0\n",
      "      - -632.0\n",
      "      - -215.0\n",
      "      - -677.0\n",
      "      - -497.0\n",
      "      - -893.0\n",
      "      - -839.0\n",
      "      - -560.0\n",
      "      - -587.0\n",
      "      - -524.0\n",
      "      - -247.0\n",
      "      - -614.0\n",
      "      - -533.0\n",
      "      - -695.0\n",
      "      - -767.0\n",
      "      - -704.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.071109897960592\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.15156862722396147\n",
      "      mean_inference_ms: 1.2997260123593022\n",
      "      mean_raw_obs_processing_ms: 0.20338857743990923\n",
      "  time_since_restore: 96.98140907287598\n",
      "  time_this_iter_s: 20.070009231567383\n",
      "  time_total_s: 96.98140907287598\n",
      "  timers:\n",
      "    learn_throughput: 229.074\n",
      "    learn_time_ms: 17461.606\n",
      "    load_throughput: 731588.045\n",
      "    load_time_ms: 5.468\n",
      "    training_iteration_time_ms: 19381.874\n",
      "    update_time_ms: 5.794\n",
      "  timestamp: 1657259532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:52:12 (running for 00:01:53.19)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         96.9814</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -707.4</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">            192.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:52:17 (running for 00:01:58.24)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         96.9814</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -707.4</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">            192.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:52:22 (running for 00:02:03.25)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         96.9814</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -707.4</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">            192.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:52:27 (running for 00:02:08.27)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         96.9814</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  -707.4</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">            192.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-52-31\n",
      "  done: false\n",
      "  episode_len_mean: 193.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -699.85\n",
      "  episode_reward_min: -992.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 123\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.6063096974485664\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01739487789018194\n",
      "          policy_loss: -0.0452695393033566\n",
      "          total_loss: 9.819673211087462\n",
      "          vf_explained_var: -0.006531480275174623\n",
      "          vf_loss: 9.861463797989712\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 24000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.077777777777778\n",
      "    ram_util_percent: 9.199999999999998\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07060838108188161\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14963761041236678\n",
      "    mean_inference_ms: 1.2995982025406192\n",
      "    mean_raw_obs_processing_ms: 0.20183140778878925\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 193.84\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1.0\n",
      "    episode_reward_mean: -699.85\n",
      "    episode_reward_min: -992.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 63\n",
      "      - 200\n",
      "      - 200\n",
      "      - 168\n",
      "      - 200\n",
      "      - 200\n",
      "      - 169\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 11\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 92\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 88\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 193\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      episode_reward:\n",
      "      - -992.0\n",
      "      - -776.0\n",
      "      - -713.0\n",
      "      - -740.0\n",
      "      - -704.0\n",
      "      - -875.0\n",
      "      - -677.0\n",
      "      - -875.0\n",
      "      - -911.0\n",
      "      - -704.0\n",
      "      - -731.0\n",
      "      - -830.0\n",
      "      - -677.0\n",
      "      - -749.0\n",
      "      - -740.0\n",
      "      - -866.0\n",
      "      - -821.0\n",
      "      - -632.0\n",
      "      - -740.0\n",
      "      - -803.0\n",
      "      - -695.0\n",
      "      - -821.0\n",
      "      - -767.0\n",
      "      - -776.0\n",
      "      - -857.0\n",
      "      - -722.0\n",
      "      - -767.0\n",
      "      - -812.0\n",
      "      - -677.0\n",
      "      - -794.0\n",
      "      - -758.0\n",
      "      - -659.0\n",
      "      - -848.0\n",
      "      - -740.0\n",
      "      - -875.0\n",
      "      - -677.0\n",
      "      - -722.0\n",
      "      - -731.0\n",
      "      - -758.0\n",
      "      - -740.0\n",
      "      - -749.0\n",
      "      - -857.0\n",
      "      - -731.0\n",
      "      - -686.0\n",
      "      - -695.0\n",
      "      - -641.0\n",
      "      - -186.0\n",
      "      - -722.0\n",
      "      - -686.0\n",
      "      - -426.0\n",
      "      - -695.0\n",
      "      - -839.0\n",
      "      - -616.0\n",
      "      - -740.0\n",
      "      - -866.0\n",
      "      - -749.0\n",
      "      - -620.0\n",
      "      - -740.0\n",
      "      - -722.0\n",
      "      - -776.0\n",
      "      - -524.0\n",
      "      - -749.0\n",
      "      - 1.0\n",
      "      - -740.0\n",
      "      - -731.0\n",
      "      - -632.0\n",
      "      - -215.0\n",
      "      - -677.0\n",
      "      - -497.0\n",
      "      - -893.0\n",
      "      - -839.0\n",
      "      - -560.0\n",
      "      - -587.0\n",
      "      - -524.0\n",
      "      - -247.0\n",
      "      - -614.0\n",
      "      - -533.0\n",
      "      - -695.0\n",
      "      - -767.0\n",
      "      - -704.0\n",
      "      - -686.0\n",
      "      - -749.0\n",
      "      - -830.0\n",
      "      - -686.0\n",
      "      - -542.0\n",
      "      - -605.0\n",
      "      - -424.0\n",
      "      - -650.0\n",
      "      - -821.0\n",
      "      - -569.0\n",
      "      - -866.0\n",
      "      - -659.0\n",
      "      - -794.0\n",
      "      - -695.0\n",
      "      - -434.0\n",
      "      - -947.0\n",
      "      - -686.0\n",
      "      - -551.0\n",
      "      - -623.0\n",
      "      - -947.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07060838108188161\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14963761041236678\n",
      "      mean_inference_ms: 1.2995982025406192\n",
      "      mean_raw_obs_processing_ms: 0.20183140778878925\n",
      "  time_since_restore: 115.91754746437073\n",
      "  time_this_iter_s: 18.93613839149475\n",
      "  time_total_s: 115.91754746437073\n",
      "  timers:\n",
      "    learn_throughput: 229.905\n",
      "    learn_time_ms: 17398.525\n",
      "    load_throughput: 745786.629\n",
      "    load_time_ms: 5.363\n",
      "    training_iteration_time_ms: 19306.052\n",
      "    update_time_ms: 5.782\n",
      "  timestamp: 1657259551\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:52:36 (running for 00:02:17.21)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         115.918</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> -699.85</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">            193.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:52:41 (running for 00:02:22.25)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         115.918</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> -699.85</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">            193.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:52:46 (running for 00:02:27.26)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         115.918</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> -699.85</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                -992</td><td style=\"text-align: right;\">            193.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_env_steps_sampled: 28000\n",
      "    num_env_steps_trained: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-52-51\n",
      "  done: false\n",
      "  episode_len_mean: 192.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -672.06\n",
      "  episode_reward_min: -947.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 144\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5517668376686753\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015291083890931617\n",
      "          policy_loss: -0.03673599322717036\n",
      "          total_loss: 9.803154233706895\n",
      "          vf_explained_var: 0.0011765173365992883\n",
      "          vf_loss: 9.836832031126946\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_env_steps_sampled: 28000\n",
      "    num_env_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 28000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.975862068965517\n",
      "    ram_util_percent: 9.2\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07049042669661212\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1490467655919566\n",
      "    mean_inference_ms: 1.303269539100716\n",
      "    mean_raw_obs_processing_ms: 0.20194753601650894\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 192.36\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1.0\n",
      "    episode_reward_mean: -672.06\n",
      "    episode_reward_min: -947.0\n",
      "    episodes_this_iter: 21\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 63\n",
      "      - 200\n",
      "      - 200\n",
      "      - 168\n",
      "      - 200\n",
      "      - 200\n",
      "      - 169\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 11\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 92\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 88\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 193\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 52\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      episode_reward:\n",
      "      - -821.0\n",
      "      - -767.0\n",
      "      - -776.0\n",
      "      - -857.0\n",
      "      - -722.0\n",
      "      - -767.0\n",
      "      - -812.0\n",
      "      - -677.0\n",
      "      - -794.0\n",
      "      - -758.0\n",
      "      - -659.0\n",
      "      - -848.0\n",
      "      - -740.0\n",
      "      - -875.0\n",
      "      - -677.0\n",
      "      - -722.0\n",
      "      - -731.0\n",
      "      - -758.0\n",
      "      - -740.0\n",
      "      - -749.0\n",
      "      - -857.0\n",
      "      - -731.0\n",
      "      - -686.0\n",
      "      - -695.0\n",
      "      - -641.0\n",
      "      - -186.0\n",
      "      - -722.0\n",
      "      - -686.0\n",
      "      - -426.0\n",
      "      - -695.0\n",
      "      - -839.0\n",
      "      - -616.0\n",
      "      - -740.0\n",
      "      - -866.0\n",
      "      - -749.0\n",
      "      - -620.0\n",
      "      - -740.0\n",
      "      - -722.0\n",
      "      - -776.0\n",
      "      - -524.0\n",
      "      - -749.0\n",
      "      - 1.0\n",
      "      - -740.0\n",
      "      - -731.0\n",
      "      - -632.0\n",
      "      - -215.0\n",
      "      - -677.0\n",
      "      - -497.0\n",
      "      - -893.0\n",
      "      - -839.0\n",
      "      - -560.0\n",
      "      - -587.0\n",
      "      - -524.0\n",
      "      - -247.0\n",
      "      - -614.0\n",
      "      - -533.0\n",
      "      - -695.0\n",
      "      - -767.0\n",
      "      - -704.0\n",
      "      - -686.0\n",
      "      - -749.0\n",
      "      - -830.0\n",
      "      - -686.0\n",
      "      - -542.0\n",
      "      - -605.0\n",
      "      - -424.0\n",
      "      - -650.0\n",
      "      - -821.0\n",
      "      - -569.0\n",
      "      - -866.0\n",
      "      - -659.0\n",
      "      - -794.0\n",
      "      - -695.0\n",
      "      - -434.0\n",
      "      - -947.0\n",
      "      - -686.0\n",
      "      - -551.0\n",
      "      - -623.0\n",
      "      - -947.0\n",
      "      - -830.0\n",
      "      - -578.0\n",
      "      - -695.0\n",
      "      - -560.0\n",
      "      - -920.0\n",
      "      - -722.0\n",
      "      - -704.0\n",
      "      - -596.0\n",
      "      - -605.0\n",
      "      - -103.0\n",
      "      - -596.0\n",
      "      - -551.0\n",
      "      - -659.0\n",
      "      - -623.0\n",
      "      - -731.0\n",
      "      - -659.0\n",
      "      - -659.0\n",
      "      - -605.0\n",
      "      - -830.0\n",
      "      - -668.0\n",
      "      - -578.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07049042669661212\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1490467655919566\n",
      "      mean_inference_ms: 1.303269539100716\n",
      "      mean_raw_obs_processing_ms: 0.20194753601650894\n",
      "  time_since_restore: 135.95753693580627\n",
      "  time_this_iter_s: 20.039989471435547\n",
      "  time_total_s: 135.95753693580627\n",
      "  timers:\n",
      "    learn_throughput: 228.65\n",
      "    learn_time_ms: 17494.011\n",
      "    load_throughput: 722541.879\n",
      "    load_time_ms: 5.536\n",
      "    training_iteration_time_ms: 19409.766\n",
      "    update_time_ms: 5.739\n",
      "  timestamp: 1657259571\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:52:51 (running for 00:02:32.38)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         135.958</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> -672.06</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                -947</td><td style=\"text-align: right;\">            192.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:52:56 (running for 00:02:37.39)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         135.958</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> -672.06</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                -947</td><td style=\"text-align: right;\">            192.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:53:01 (running for 00:02:42.41)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         135.958</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> -672.06</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                -947</td><td style=\"text-align: right;\">            192.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:53:06 (running for 00:02:47.42)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         135.958</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> -672.06</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                -947</td><td style=\"text-align: right;\">            192.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 32000\n",
      "    num_env_steps_trained: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-53-09\n",
      "  done: false\n",
      "  episode_len_mean: 190.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -638.74\n",
      "  episode_reward_min: -947.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 164\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5699470486692202\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019756655226010573\n",
      "          policy_loss: -0.050471311515217186\n",
      "          total_loss: 9.828002715367143\n",
      "          vf_explained_var: -0.0049945534557424565\n",
      "          vf_loss: 9.874522669597338\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 32000\n",
      "    num_env_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 32000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.104\n",
      "    ram_util_percent: 9.2\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07039199683310038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14863261968864225\n",
      "    mean_inference_ms: 1.3045965609028345\n",
      "    mean_raw_obs_processing_ms: 0.20240609810867263\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 190.69\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1.0\n",
      "    episode_reward_mean: -638.74\n",
      "    episode_reward_min: -947.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 63\n",
      "      - 200\n",
      "      - 200\n",
      "      - 168\n",
      "      - 200\n",
      "      - 200\n",
      "      - 169\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 11\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 92\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 88\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 193\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 52\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 152\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 81\n",
      "      - 200\n",
      "      episode_reward:\n",
      "      - -857.0\n",
      "      - -731.0\n",
      "      - -686.0\n",
      "      - -695.0\n",
      "      - -641.0\n",
      "      - -186.0\n",
      "      - -722.0\n",
      "      - -686.0\n",
      "      - -426.0\n",
      "      - -695.0\n",
      "      - -839.0\n",
      "      - -616.0\n",
      "      - -740.0\n",
      "      - -866.0\n",
      "      - -749.0\n",
      "      - -620.0\n",
      "      - -740.0\n",
      "      - -722.0\n",
      "      - -776.0\n",
      "      - -524.0\n",
      "      - -749.0\n",
      "      - 1.0\n",
      "      - -740.0\n",
      "      - -731.0\n",
      "      - -632.0\n",
      "      - -215.0\n",
      "      - -677.0\n",
      "      - -497.0\n",
      "      - -893.0\n",
      "      - -839.0\n",
      "      - -560.0\n",
      "      - -587.0\n",
      "      - -524.0\n",
      "      - -247.0\n",
      "      - -614.0\n",
      "      - -533.0\n",
      "      - -695.0\n",
      "      - -767.0\n",
      "      - -704.0\n",
      "      - -686.0\n",
      "      - -749.0\n",
      "      - -830.0\n",
      "      - -686.0\n",
      "      - -542.0\n",
      "      - -605.0\n",
      "      - -424.0\n",
      "      - -650.0\n",
      "      - -821.0\n",
      "      - -569.0\n",
      "      - -866.0\n",
      "      - -659.0\n",
      "      - -794.0\n",
      "      - -695.0\n",
      "      - -434.0\n",
      "      - -947.0\n",
      "      - -686.0\n",
      "      - -551.0\n",
      "      - -623.0\n",
      "      - -947.0\n",
      "      - -830.0\n",
      "      - -578.0\n",
      "      - -695.0\n",
      "      - -560.0\n",
      "      - -920.0\n",
      "      - -722.0\n",
      "      - -704.0\n",
      "      - -596.0\n",
      "      - -605.0\n",
      "      - -103.0\n",
      "      - -596.0\n",
      "      - -551.0\n",
      "      - -659.0\n",
      "      - -623.0\n",
      "      - -731.0\n",
      "      - -659.0\n",
      "      - -659.0\n",
      "      - -605.0\n",
      "      - -830.0\n",
      "      - -668.0\n",
      "      - -578.0\n",
      "      - -632.0\n",
      "      - -605.0\n",
      "      - -722.0\n",
      "      - -632.0\n",
      "      - -560.0\n",
      "      - -389.0\n",
      "      - -695.0\n",
      "      - -686.0\n",
      "      - -497.0\n",
      "      - -659.0\n",
      "      - -875.0\n",
      "      - -506.0\n",
      "      - -758.0\n",
      "      - -464.0\n",
      "      - -524.0\n",
      "      - -641.0\n",
      "      - -632.0\n",
      "      - -641.0\n",
      "      - -150.0\n",
      "      - -650.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07039199683310038\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14863261968864225\n",
      "      mean_inference_ms: 1.3045965609028345\n",
      "      mean_raw_obs_processing_ms: 0.20240609810867263\n",
      "  time_since_restore: 153.75404906272888\n",
      "  time_this_iter_s: 17.796512126922607\n",
      "  time_total_s: 153.75404906272888\n",
      "  timers:\n",
      "    learn_throughput: 231.348\n",
      "    learn_time_ms: 17290.005\n",
      "    load_throughput: 714456.127\n",
      "    load_time_ms: 5.599\n",
      "    training_iteration_time_ms: 19206.929\n",
      "    update_time_ms: 5.817\n",
      "  timestamp: 1657259589\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:53:14 (running for 00:02:55.26)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         153.754</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> -638.74</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                -947</td><td style=\"text-align: right;\">            190.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:53:19 (running for 00:03:00.27)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         153.754</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> -638.74</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                -947</td><td style=\"text-align: right;\">            190.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:53:24 (running for 00:03:05.29)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         153.754</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> -638.74</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                -947</td><td style=\"text-align: right;\">            190.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 36000\n",
      "    num_env_steps_trained: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-53-29\n",
      "  done: false\n",
      "  episode_len_mean: 190.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -103.0\n",
      "  episode_reward_mean: -610.57\n",
      "  episode_reward_min: -947.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 186\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5202861071914755\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016428695617064674\n",
      "          policy_loss: -0.04578051389225068\n",
      "          total_loss: 9.744190662137923\n",
      "          vf_explained_var: -0.00655690130367074\n",
      "          vf_loss: 9.786685432926301\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_env_steps_sampled: 36000\n",
      "    num_env_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 36000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.262068965517242\n",
      "    ram_util_percent: 9.2\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0703413102917054\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14833072101370348\n",
      "    mean_inference_ms: 1.3058206243028316\n",
      "    mean_raw_obs_processing_ms: 0.20287803409315408\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 190.54\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -103.0\n",
      "    episode_reward_mean: -610.57\n",
      "    episode_reward_min: -947.0\n",
      "    episodes_this_iter: 22\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 92\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 88\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 193\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 52\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 152\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 81\n",
      "      - 200\n",
      "      - 147\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 135\n",
      "      - 72\n",
      "      - 191\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 120\n",
      "      - 200\n",
      "      - 131\n",
      "      episode_reward:\n",
      "      - -740.0\n",
      "      - -731.0\n",
      "      - -632.0\n",
      "      - -215.0\n",
      "      - -677.0\n",
      "      - -497.0\n",
      "      - -893.0\n",
      "      - -839.0\n",
      "      - -560.0\n",
      "      - -587.0\n",
      "      - -524.0\n",
      "      - -247.0\n",
      "      - -614.0\n",
      "      - -533.0\n",
      "      - -695.0\n",
      "      - -767.0\n",
      "      - -704.0\n",
      "      - -686.0\n",
      "      - -749.0\n",
      "      - -830.0\n",
      "      - -686.0\n",
      "      - -542.0\n",
      "      - -605.0\n",
      "      - -424.0\n",
      "      - -650.0\n",
      "      - -821.0\n",
      "      - -569.0\n",
      "      - -866.0\n",
      "      - -659.0\n",
      "      - -794.0\n",
      "      - -695.0\n",
      "      - -434.0\n",
      "      - -947.0\n",
      "      - -686.0\n",
      "      - -551.0\n",
      "      - -623.0\n",
      "      - -947.0\n",
      "      - -830.0\n",
      "      - -578.0\n",
      "      - -695.0\n",
      "      - -560.0\n",
      "      - -920.0\n",
      "      - -722.0\n",
      "      - -704.0\n",
      "      - -596.0\n",
      "      - -605.0\n",
      "      - -103.0\n",
      "      - -596.0\n",
      "      - -551.0\n",
      "      - -659.0\n",
      "      - -623.0\n",
      "      - -731.0\n",
      "      - -659.0\n",
      "      - -659.0\n",
      "      - -605.0\n",
      "      - -830.0\n",
      "      - -668.0\n",
      "      - -578.0\n",
      "      - -632.0\n",
      "      - -605.0\n",
      "      - -722.0\n",
      "      - -632.0\n",
      "      - -560.0\n",
      "      - -389.0\n",
      "      - -695.0\n",
      "      - -686.0\n",
      "      - -497.0\n",
      "      - -659.0\n",
      "      - -875.0\n",
      "      - -506.0\n",
      "      - -758.0\n",
      "      - -464.0\n",
      "      - -524.0\n",
      "      - -641.0\n",
      "      - -632.0\n",
      "      - -641.0\n",
      "      - -150.0\n",
      "      - -650.0\n",
      "      - -432.0\n",
      "      - -434.0\n",
      "      - -470.0\n",
      "      - -506.0\n",
      "      - -366.0\n",
      "      - -150.0\n",
      "      - -539.0\n",
      "      - -704.0\n",
      "      - -560.0\n",
      "      - -605.0\n",
      "      - -533.0\n",
      "      - -641.0\n",
      "      - -587.0\n",
      "      - -623.0\n",
      "      - -506.0\n",
      "      - -596.0\n",
      "      - -776.0\n",
      "      - -587.0\n",
      "      - -596.0\n",
      "      - -396.0\n",
      "      - -542.0\n",
      "      - -299.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0703413102917054\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14833072101370348\n",
      "      mean_inference_ms: 1.3058206243028316\n",
      "      mean_raw_obs_processing_ms: 0.20287803409315408\n",
      "  time_since_restore: 173.61002898216248\n",
      "  time_this_iter_s: 19.855979919433594\n",
      "  time_total_s: 173.61002898216248\n",
      "  timers:\n",
      "    learn_throughput: 230.459\n",
      "    learn_time_ms: 17356.69\n",
      "    load_throughput: 702854.541\n",
      "    load_time_ms: 5.691\n",
      "    training_iteration_time_ms: 19277.996\n",
      "    update_time_ms: 5.767\n",
      "  timestamp: 1657259609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:53:34 (running for 00:03:15.15)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">          173.61</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> -610.57</td><td style=\"text-align: right;\">                -103</td><td style=\"text-align: right;\">                -947</td><td style=\"text-align: right;\">            190.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:53:39 (running for 00:03:20.19)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">          173.61</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> -610.57</td><td style=\"text-align: right;\">                -103</td><td style=\"text-align: right;\">                -947</td><td style=\"text-align: right;\">            190.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:53:44 (running for 00:03:25.19)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">          173.61</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> -610.57</td><td style=\"text-align: right;\">                -103</td><td style=\"text-align: right;\">                -947</td><td style=\"text-align: right;\">            190.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 40000\n",
      "    num_env_steps_trained: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-53-44\n",
      "  done: false\n",
      "  episode_len_mean: 188.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -103.0\n",
      "  episode_reward_mean: -582.16\n",
      "  episode_reward_min: -947.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 207\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.441570834703343\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021028203042513547\n",
      "          policy_loss: -0.04576854933333653\n",
      "          total_loss: 9.721857324210546\n",
      "          vf_explained_var: -0.01445419551223837\n",
      "          vf_loss: 9.763420242391607\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 40000\n",
      "    num_env_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 40000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.530434782608697\n",
      "    ram_util_percent: 9.199999999999996\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07029220317837782\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14796589699943616\n",
      "    mean_inference_ms: 1.3053818257907193\n",
      "    mean_raw_obs_processing_ms: 0.20293880079644314\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 188.77\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -103.0\n",
      "    episode_reward_mean: -582.16\n",
      "    episode_reward_min: -947.0\n",
      "    episodes_this_iter: 21\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 200\n",
      "      - 200\n",
      "      - 193\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 52\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 152\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 81\n",
      "      - 200\n",
      "      - 147\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 135\n",
      "      - 72\n",
      "      - 191\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 120\n",
      "      - 200\n",
      "      - 131\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 78\n",
      "      - 200\n",
      "      - 200\n",
      "      - 158\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 105\n",
      "      - 116\n",
      "      - 146\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      episode_reward:\n",
      "      - -542.0\n",
      "      - -605.0\n",
      "      - -424.0\n",
      "      - -650.0\n",
      "      - -821.0\n",
      "      - -569.0\n",
      "      - -866.0\n",
      "      - -659.0\n",
      "      - -794.0\n",
      "      - -695.0\n",
      "      - -434.0\n",
      "      - -947.0\n",
      "      - -686.0\n",
      "      - -551.0\n",
      "      - -623.0\n",
      "      - -947.0\n",
      "      - -830.0\n",
      "      - -578.0\n",
      "      - -695.0\n",
      "      - -560.0\n",
      "      - -920.0\n",
      "      - -722.0\n",
      "      - -704.0\n",
      "      - -596.0\n",
      "      - -605.0\n",
      "      - -103.0\n",
      "      - -596.0\n",
      "      - -551.0\n",
      "      - -659.0\n",
      "      - -623.0\n",
      "      - -731.0\n",
      "      - -659.0\n",
      "      - -659.0\n",
      "      - -605.0\n",
      "      - -830.0\n",
      "      - -668.0\n",
      "      - -578.0\n",
      "      - -632.0\n",
      "      - -605.0\n",
      "      - -722.0\n",
      "      - -632.0\n",
      "      - -560.0\n",
      "      - -389.0\n",
      "      - -695.0\n",
      "      - -686.0\n",
      "      - -497.0\n",
      "      - -659.0\n",
      "      - -875.0\n",
      "      - -506.0\n",
      "      - -758.0\n",
      "      - -464.0\n",
      "      - -524.0\n",
      "      - -641.0\n",
      "      - -632.0\n",
      "      - -641.0\n",
      "      - -150.0\n",
      "      - -650.0\n",
      "      - -432.0\n",
      "      - -434.0\n",
      "      - -470.0\n",
      "      - -506.0\n",
      "      - -366.0\n",
      "      - -150.0\n",
      "      - -539.0\n",
      "      - -704.0\n",
      "      - -560.0\n",
      "      - -605.0\n",
      "      - -533.0\n",
      "      - -641.0\n",
      "      - -587.0\n",
      "      - -623.0\n",
      "      - -506.0\n",
      "      - -596.0\n",
      "      - -776.0\n",
      "      - -587.0\n",
      "      - -596.0\n",
      "      - -396.0\n",
      "      - -542.0\n",
      "      - -299.0\n",
      "      - -569.0\n",
      "      - -614.0\n",
      "      - -362.0\n",
      "      - -147.0\n",
      "      - -641.0\n",
      "      - -452.0\n",
      "      - -389.0\n",
      "      - -569.0\n",
      "      - -731.0\n",
      "      - -533.0\n",
      "      - -605.0\n",
      "      - -318.0\n",
      "      - -140.0\n",
      "      - -332.0\n",
      "      - -749.0\n",
      "      - -515.0\n",
      "      - -668.0\n",
      "      - -515.0\n",
      "      - -461.0\n",
      "      - -731.0\n",
      "      - -524.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07029220317837782\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14796589699943616\n",
      "      mean_inference_ms: 1.3053818257907193\n",
      "      mean_raw_obs_processing_ms: 0.20293880079644314\n",
      "  time_since_restore: 189.46363186836243\n",
      "  time_this_iter_s: 15.853602886199951\n",
      "  time_total_s: 189.46363186836243\n",
      "  timers:\n",
      "    learn_throughput: 235.079\n",
      "    learn_time_ms: 17015.568\n",
      "    load_throughput: 714830.551\n",
      "    load_time_ms: 5.596\n",
      "    training_iteration_time_ms: 18934.71\n",
      "    update_time_ms: 5.662\n",
      "  timestamp: 1657259624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:53:50 (running for 00:03:31.12)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         189.464</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> -582.16</td><td style=\"text-align: right;\">                -103</td><td style=\"text-align: right;\">                -947</td><td style=\"text-align: right;\">            188.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:53:55 (running for 00:03:36.12)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         189.464</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> -582.16</td><td style=\"text-align: right;\">                -103</td><td style=\"text-align: right;\">                -947</td><td style=\"text-align: right;\">            188.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:54:00 (running for 00:03:41.15)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         189.464</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> -582.16</td><td style=\"text-align: right;\">                -103</td><td style=\"text-align: right;\">                -947</td><td style=\"text-align: right;\">            188.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_env_steps_sampled: 44000\n",
      "    num_env_steps_trained: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-54-04\n",
      "  done: false\n",
      "  episode_len_mean: 181.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -7.0\n",
      "  episode_reward_mean: -530.0\n",
      "  episode_reward_min: -875.0\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 231\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.4191789801402759\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01774508064150573\n",
      "          policy_loss: -0.043029183958486844\n",
      "          total_loss: 9.706453352077032\n",
      "          vf_explained_var: -0.0009574598202141383\n",
      "          vf_loss: 9.744159033990675\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_env_steps_sampled: 44000\n",
      "    num_env_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 44000\n",
      "  num_agent_steps_trained: 44000\n",
      "  num_env_steps_sampled: 44000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 44000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.303703703703705\n",
      "    ram_util_percent: 9.199999999999998\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07036017721491386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14782209724680795\n",
      "    mean_inference_ms: 1.3067068772656314\n",
      "    mean_raw_obs_processing_ms: 0.20327125208551627\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 181.52\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -7.0\n",
      "    episode_reward_mean: -530.0\n",
      "    episode_reward_min: -875.0\n",
      "    episodes_this_iter: 24\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 200\n",
      "      - 52\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 152\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 81\n",
      "      - 200\n",
      "      - 147\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 135\n",
      "      - 72\n",
      "      - 191\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 120\n",
      "      - 200\n",
      "      - 131\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 78\n",
      "      - 200\n",
      "      - 200\n",
      "      - 158\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 105\n",
      "      - 116\n",
      "      - 146\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 51\n",
      "      - 147\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 52\n",
      "      - 200\n",
      "      - 200\n",
      "      - 105\n",
      "      - 200\n",
      "      - 200\n",
      "      - 140\n",
      "      - 200\n",
      "      - 154\n",
      "      - 200\n",
      "      - 200\n",
      "      - 19\n",
      "      episode_reward:\n",
      "      - -605.0\n",
      "      - -103.0\n",
      "      - -596.0\n",
      "      - -551.0\n",
      "      - -659.0\n",
      "      - -623.0\n",
      "      - -731.0\n",
      "      - -659.0\n",
      "      - -659.0\n",
      "      - -605.0\n",
      "      - -830.0\n",
      "      - -668.0\n",
      "      - -578.0\n",
      "      - -632.0\n",
      "      - -605.0\n",
      "      - -722.0\n",
      "      - -632.0\n",
      "      - -560.0\n",
      "      - -389.0\n",
      "      - -695.0\n",
      "      - -686.0\n",
      "      - -497.0\n",
      "      - -659.0\n",
      "      - -875.0\n",
      "      - -506.0\n",
      "      - -758.0\n",
      "      - -464.0\n",
      "      - -524.0\n",
      "      - -641.0\n",
      "      - -632.0\n",
      "      - -641.0\n",
      "      - -150.0\n",
      "      - -650.0\n",
      "      - -432.0\n",
      "      - -434.0\n",
      "      - -470.0\n",
      "      - -506.0\n",
      "      - -366.0\n",
      "      - -150.0\n",
      "      - -539.0\n",
      "      - -704.0\n",
      "      - -560.0\n",
      "      - -605.0\n",
      "      - -533.0\n",
      "      - -641.0\n",
      "      - -587.0\n",
      "      - -623.0\n",
      "      - -506.0\n",
      "      - -596.0\n",
      "      - -776.0\n",
      "      - -587.0\n",
      "      - -596.0\n",
      "      - -396.0\n",
      "      - -542.0\n",
      "      - -299.0\n",
      "      - -569.0\n",
      "      - -614.0\n",
      "      - -362.0\n",
      "      - -147.0\n",
      "      - -641.0\n",
      "      - -452.0\n",
      "      - -389.0\n",
      "      - -569.0\n",
      "      - -731.0\n",
      "      - -533.0\n",
      "      - -605.0\n",
      "      - -318.0\n",
      "      - -140.0\n",
      "      - -332.0\n",
      "      - -749.0\n",
      "      - -515.0\n",
      "      - -668.0\n",
      "      - -515.0\n",
      "      - -461.0\n",
      "      - -731.0\n",
      "      - -524.0\n",
      "      - -659.0\n",
      "      - -812.0\n",
      "      - -93.0\n",
      "      - -207.0\n",
      "      - -641.0\n",
      "      - -605.0\n",
      "      - -488.0\n",
      "      - -578.0\n",
      "      - -524.0\n",
      "      - -515.0\n",
      "      - -506.0\n",
      "      - -506.0\n",
      "      - -40.0\n",
      "      - -515.0\n",
      "      - -776.0\n",
      "      - -192.0\n",
      "      - -524.0\n",
      "      - -749.0\n",
      "      - -236.0\n",
      "      - -380.0\n",
      "      - -412.0\n",
      "      - -677.0\n",
      "      - -560.0\n",
      "      - -7.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07036017721491386\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14782209724680795\n",
      "      mean_inference_ms: 1.3067068772656314\n",
      "      mean_raw_obs_processing_ms: 0.20327125208551627\n",
      "  time_since_restore: 208.87704706192017\n",
      "  time_this_iter_s: 19.41341519355774\n",
      "  time_total_s: 208.87704706192017\n",
      "  timers:\n",
      "    learn_throughput: 237.175\n",
      "    learn_time_ms: 16865.218\n",
      "    load_throughput: 703040.421\n",
      "    load_time_ms: 5.69\n",
      "    training_iteration_time_ms: 18793.823\n",
      "    update_time_ms: 5.735\n",
      "  timestamp: 1657259644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:54:09 (running for 00:03:50.55)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         208.877</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">    -530</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -875</td><td style=\"text-align: right;\">            181.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:54:14 (running for 00:03:55.61)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         208.877</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">    -530</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -875</td><td style=\"text-align: right;\">            181.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:54:19 (running for 00:04:00.61)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         208.877</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">    -530</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -875</td><td style=\"text-align: right;\">            181.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 48000\n",
      "    num_env_steps_trained: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-54-23\n",
      "  done: false\n",
      "  episode_len_mean: 175.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -7.0\n",
      "  episode_reward_mean: -486.01\n",
      "  episode_reward_min: -812.0\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 256\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.3725370958928138\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01842762184769469\n",
      "          policy_loss: -0.046458250524536254\n",
      "          total_loss: 9.666787348511399\n",
      "          vf_explained_var: -0.007567577528697188\n",
      "          vf_loss: 9.70771726690313\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 48000\n",
      "    num_env_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 48000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 48000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.139285714285713\n",
      "    ram_util_percent: 9.2\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07034381597626203\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14753168774522551\n",
      "    mean_inference_ms: 1.3056356776352138\n",
      "    mean_raw_obs_processing_ms: 0.20421851934423949\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 175.69\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -7.0\n",
      "    episode_reward_mean: -486.01\n",
      "    episode_reward_min: -812.0\n",
      "    episodes_this_iter: 25\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 200\n",
      "      - 152\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 81\n",
      "      - 200\n",
      "      - 147\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 135\n",
      "      - 72\n",
      "      - 191\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 120\n",
      "      - 200\n",
      "      - 131\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 78\n",
      "      - 200\n",
      "      - 200\n",
      "      - 158\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 105\n",
      "      - 116\n",
      "      - 146\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 51\n",
      "      - 147\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 52\n",
      "      - 200\n",
      "      - 200\n",
      "      - 105\n",
      "      - 200\n",
      "      - 200\n",
      "      - 140\n",
      "      - 200\n",
      "      - 154\n",
      "      - 200\n",
      "      - 200\n",
      "      - 19\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 121\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 74\n",
      "      - 138\n",
      "      - 200\n",
      "      - 200\n",
      "      - 49\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 109\n",
      "      - 60\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 118\n",
      "      - 200\n",
      "      episode_reward:\n",
      "      - -758.0\n",
      "      - -464.0\n",
      "      - -524.0\n",
      "      - -641.0\n",
      "      - -632.0\n",
      "      - -641.0\n",
      "      - -150.0\n",
      "      - -650.0\n",
      "      - -432.0\n",
      "      - -434.0\n",
      "      - -470.0\n",
      "      - -506.0\n",
      "      - -366.0\n",
      "      - -150.0\n",
      "      - -539.0\n",
      "      - -704.0\n",
      "      - -560.0\n",
      "      - -605.0\n",
      "      - -533.0\n",
      "      - -641.0\n",
      "      - -587.0\n",
      "      - -623.0\n",
      "      - -506.0\n",
      "      - -596.0\n",
      "      - -776.0\n",
      "      - -587.0\n",
      "      - -596.0\n",
      "      - -396.0\n",
      "      - -542.0\n",
      "      - -299.0\n",
      "      - -569.0\n",
      "      - -614.0\n",
      "      - -362.0\n",
      "      - -147.0\n",
      "      - -641.0\n",
      "      - -452.0\n",
      "      - -389.0\n",
      "      - -569.0\n",
      "      - -731.0\n",
      "      - -533.0\n",
      "      - -605.0\n",
      "      - -318.0\n",
      "      - -140.0\n",
      "      - -332.0\n",
      "      - -749.0\n",
      "      - -515.0\n",
      "      - -668.0\n",
      "      - -515.0\n",
      "      - -461.0\n",
      "      - -731.0\n",
      "      - -524.0\n",
      "      - -659.0\n",
      "      - -812.0\n",
      "      - -93.0\n",
      "      - -207.0\n",
      "      - -641.0\n",
      "      - -605.0\n",
      "      - -488.0\n",
      "      - -578.0\n",
      "      - -524.0\n",
      "      - -515.0\n",
      "      - -506.0\n",
      "      - -506.0\n",
      "      - -40.0\n",
      "      - -515.0\n",
      "      - -776.0\n",
      "      - -192.0\n",
      "      - -524.0\n",
      "      - -749.0\n",
      "      - -236.0\n",
      "      - -380.0\n",
      "      - -412.0\n",
      "      - -677.0\n",
      "      - -560.0\n",
      "      - -7.0\n",
      "      - -551.0\n",
      "      - -677.0\n",
      "      - -353.0\n",
      "      - -578.0\n",
      "      - -380.0\n",
      "      - -280.0\n",
      "      - -542.0\n",
      "      - -560.0\n",
      "      - -290.0\n",
      "      - -152.0\n",
      "      - -387.0\n",
      "      - -641.0\n",
      "      - -470.0\n",
      "      - -55.0\n",
      "      - -515.0\n",
      "      - -623.0\n",
      "      - -560.0\n",
      "      - -178.0\n",
      "      - -111.0\n",
      "      - -686.0\n",
      "      - -542.0\n",
      "      - -542.0\n",
      "      - -587.0\n",
      "      - -268.0\n",
      "      - -398.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07034381597626203\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14753168774522551\n",
      "      mean_inference_ms: 1.3056356776352138\n",
      "      mean_raw_obs_processing_ms: 0.20421851934423949\n",
      "  time_since_restore: 228.09981656074524\n",
      "  time_this_iter_s: 19.222769498825073\n",
      "  time_total_s: 228.09981656074524\n",
      "  timers:\n",
      "    learn_throughput: 234.05\n",
      "    learn_time_ms: 17090.399\n",
      "    load_throughput: 697466.836\n",
      "    load_time_ms: 5.735\n",
      "    training_iteration_time_ms: 19023.767\n",
      "    update_time_ms: 5.834\n",
      "  timestamp: 1657259663\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:54:28 (running for 00:04:09.88)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">           228.1</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -486.01</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -812</td><td style=\"text-align: right;\">            175.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:54:33 (running for 00:04:14.89)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">           228.1</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -486.01</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -812</td><td style=\"text-align: right;\">            175.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:54:38 (running for 00:04:19.91)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">           228.1</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -486.01</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -812</td><td style=\"text-align: right;\">            175.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_env_steps_sampled: 52000\n",
      "    num_env_steps_trained: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-54-43\n",
      "  done: false\n",
      "  episode_len_mean: 179.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -7.0\n",
      "  episode_reward_mean: -482.13\n",
      "  episode_reward_min: -812.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 276\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.293533303083912\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017980076153063324\n",
      "          policy_loss: -0.03469065641683917\n",
      "          total_loss: 9.721818778335408\n",
      "          vf_explained_var: -0.034611613199275026\n",
      "          vf_loss: 9.751115405174994\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_env_steps_sampled: 52000\n",
      "    num_env_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 52000\n",
      "  num_agent_steps_trained: 52000\n",
      "  num_env_steps_sampled: 52000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 52000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.196428571428572\n",
      "    ram_util_percent: 9.2\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07025954636668512\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14723260388087347\n",
      "    mean_inference_ms: 1.3041956843545197\n",
      "    mean_raw_obs_processing_ms: 0.20461591851076563\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 179.91\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -7.0\n",
      "    episode_reward_mean: -482.13\n",
      "    episode_reward_min: -812.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 120\n",
      "      - 200\n",
      "      - 131\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 78\n",
      "      - 200\n",
      "      - 200\n",
      "      - 158\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 105\n",
      "      - 116\n",
      "      - 146\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 51\n",
      "      - 147\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 52\n",
      "      - 200\n",
      "      - 200\n",
      "      - 105\n",
      "      - 200\n",
      "      - 200\n",
      "      - 140\n",
      "      - 200\n",
      "      - 154\n",
      "      - 200\n",
      "      - 200\n",
      "      - 19\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 121\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 74\n",
      "      - 138\n",
      "      - 200\n",
      "      - 200\n",
      "      - 49\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 109\n",
      "      - 60\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 118\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      episode_reward:\n",
      "      - -587.0\n",
      "      - -623.0\n",
      "      - -506.0\n",
      "      - -596.0\n",
      "      - -776.0\n",
      "      - -587.0\n",
      "      - -596.0\n",
      "      - -396.0\n",
      "      - -542.0\n",
      "      - -299.0\n",
      "      - -569.0\n",
      "      - -614.0\n",
      "      - -362.0\n",
      "      - -147.0\n",
      "      - -641.0\n",
      "      - -452.0\n",
      "      - -389.0\n",
      "      - -569.0\n",
      "      - -731.0\n",
      "      - -533.0\n",
      "      - -605.0\n",
      "      - -318.0\n",
      "      - -140.0\n",
      "      - -332.0\n",
      "      - -749.0\n",
      "      - -515.0\n",
      "      - -668.0\n",
      "      - -515.0\n",
      "      - -461.0\n",
      "      - -731.0\n",
      "      - -524.0\n",
      "      - -659.0\n",
      "      - -812.0\n",
      "      - -93.0\n",
      "      - -207.0\n",
      "      - -641.0\n",
      "      - -605.0\n",
      "      - -488.0\n",
      "      - -578.0\n",
      "      - -524.0\n",
      "      - -515.0\n",
      "      - -506.0\n",
      "      - -506.0\n",
      "      - -40.0\n",
      "      - -515.0\n",
      "      - -776.0\n",
      "      - -192.0\n",
      "      - -524.0\n",
      "      - -749.0\n",
      "      - -236.0\n",
      "      - -380.0\n",
      "      - -412.0\n",
      "      - -677.0\n",
      "      - -560.0\n",
      "      - -7.0\n",
      "      - -551.0\n",
      "      - -677.0\n",
      "      - -353.0\n",
      "      - -578.0\n",
      "      - -380.0\n",
      "      - -280.0\n",
      "      - -542.0\n",
      "      - -560.0\n",
      "      - -290.0\n",
      "      - -152.0\n",
      "      - -387.0\n",
      "      - -641.0\n",
      "      - -470.0\n",
      "      - -55.0\n",
      "      - -515.0\n",
      "      - -623.0\n",
      "      - -560.0\n",
      "      - -178.0\n",
      "      - -111.0\n",
      "      - -686.0\n",
      "      - -542.0\n",
      "      - -542.0\n",
      "      - -587.0\n",
      "      - -268.0\n",
      "      - -398.0\n",
      "      - -605.0\n",
      "      - -452.0\n",
      "      - -587.0\n",
      "      - -623.0\n",
      "      - -308.0\n",
      "      - -317.0\n",
      "      - -668.0\n",
      "      - -479.0\n",
      "      - -443.0\n",
      "      - -479.0\n",
      "      - -434.0\n",
      "      - -749.0\n",
      "      - -506.0\n",
      "      - -506.0\n",
      "      - -479.0\n",
      "      - -533.0\n",
      "      - -371.0\n",
      "      - -587.0\n",
      "      - -380.0\n",
      "      - -506.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07025954636668512\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14723260388087347\n",
      "      mean_inference_ms: 1.3041956843545197\n",
      "      mean_raw_obs_processing_ms: 0.20461591851076563\n",
      "  time_since_restore: 247.95629596710205\n",
      "  time_this_iter_s: 19.85647940635681\n",
      "  time_total_s: 247.95629596710205\n",
      "  timers:\n",
      "    learn_throughput: 234.782\n",
      "    learn_time_ms: 17037.076\n",
      "    load_throughput: 677008.401\n",
      "    load_time_ms: 5.908\n",
      "    training_iteration_time_ms: 18971.653\n",
      "    update_time_ms: 5.849\n",
      "  timestamp: 1657259683\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:54:48 (running for 00:04:29.77)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         247.956</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -482.13</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -812</td><td style=\"text-align: right;\">            179.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:54:53 (running for 00:04:34.83)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         247.956</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -482.13</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -812</td><td style=\"text-align: right;\">            179.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:54:58 (running for 00:04:39.84)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         247.956</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -482.13</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -812</td><td style=\"text-align: right;\">            179.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:55:03 (running for 00:04:44.86)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         247.956</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -482.13</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -812</td><td style=\"text-align: right;\">            179.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_env_steps_sampled: 56000\n",
      "    num_env_steps_trained: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-55-04\n",
      "  done: false\n",
      "  episode_len_mean: 177.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -7.0\n",
      "  episode_reward_mean: -435.8\n",
      "  episode_reward_min: -812.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 299\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1546392670241736\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015396402288773566\n",
      "          policy_loss: -0.03685002359411409\n",
      "          total_loss: 9.64412277590844\n",
      "          vf_explained_var: -0.030009642435658363\n",
      "          vf_loss: 9.676353892459664\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_env_steps_sampled: 56000\n",
      "    num_env_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 56000\n",
      "  num_agent_steps_trained: 56000\n",
      "  num_env_steps_sampled: 56000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 56000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.1068965517241365\n",
      "    ram_util_percent: 9.2\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07014011990417605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14686249145600067\n",
      "    mean_inference_ms: 1.3019433500487048\n",
      "    mean_raw_obs_processing_ms: 0.20525897084483571\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 177.14\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -7.0\n",
      "    episode_reward_mean: -435.8\n",
      "    episode_reward_min: -812.0\n",
      "    episodes_this_iter: 23\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 146\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 51\n",
      "      - 147\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 52\n",
      "      - 200\n",
      "      - 200\n",
      "      - 105\n",
      "      - 200\n",
      "      - 200\n",
      "      - 140\n",
      "      - 200\n",
      "      - 154\n",
      "      - 200\n",
      "      - 200\n",
      "      - 19\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 121\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 74\n",
      "      - 138\n",
      "      - 200\n",
      "      - 200\n",
      "      - 49\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 109\n",
      "      - 60\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 118\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 22\n",
      "      - 36\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 189\n",
      "      - 200\n",
      "      - 200\n",
      "      - 24\n",
      "      - 116\n",
      "      - 44\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      episode_reward:\n",
      "      - -332.0\n",
      "      - -749.0\n",
      "      - -515.0\n",
      "      - -668.0\n",
      "      - -515.0\n",
      "      - -461.0\n",
      "      - -731.0\n",
      "      - -524.0\n",
      "      - -659.0\n",
      "      - -812.0\n",
      "      - -93.0\n",
      "      - -207.0\n",
      "      - -641.0\n",
      "      - -605.0\n",
      "      - -488.0\n",
      "      - -578.0\n",
      "      - -524.0\n",
      "      - -515.0\n",
      "      - -506.0\n",
      "      - -506.0\n",
      "      - -40.0\n",
      "      - -515.0\n",
      "      - -776.0\n",
      "      - -192.0\n",
      "      - -524.0\n",
      "      - -749.0\n",
      "      - -236.0\n",
      "      - -380.0\n",
      "      - -412.0\n",
      "      - -677.0\n",
      "      - -560.0\n",
      "      - -7.0\n",
      "      - -551.0\n",
      "      - -677.0\n",
      "      - -353.0\n",
      "      - -578.0\n",
      "      - -380.0\n",
      "      - -280.0\n",
      "      - -542.0\n",
      "      - -560.0\n",
      "      - -290.0\n",
      "      - -152.0\n",
      "      - -387.0\n",
      "      - -641.0\n",
      "      - -470.0\n",
      "      - -55.0\n",
      "      - -515.0\n",
      "      - -623.0\n",
      "      - -560.0\n",
      "      - -178.0\n",
      "      - -111.0\n",
      "      - -686.0\n",
      "      - -542.0\n",
      "      - -542.0\n",
      "      - -587.0\n",
      "      - -268.0\n",
      "      - -398.0\n",
      "      - -605.0\n",
      "      - -452.0\n",
      "      - -587.0\n",
      "      - -623.0\n",
      "      - -308.0\n",
      "      - -317.0\n",
      "      - -668.0\n",
      "      - -479.0\n",
      "      - -443.0\n",
      "      - -479.0\n",
      "      - -434.0\n",
      "      - -749.0\n",
      "      - -506.0\n",
      "      - -506.0\n",
      "      - -479.0\n",
      "      - -533.0\n",
      "      - -371.0\n",
      "      - -587.0\n",
      "      - -380.0\n",
      "      - -506.0\n",
      "      - -281.0\n",
      "      - -272.0\n",
      "      - -479.0\n",
      "      - -344.0\n",
      "      - -10.0\n",
      "      - -60.0\n",
      "      - -425.0\n",
      "      - -425.0\n",
      "      - -578.0\n",
      "      - -317.0\n",
      "      - -236.0\n",
      "      - -272.0\n",
      "      - -407.0\n",
      "      - -317.0\n",
      "      - -420.0\n",
      "      - -290.0\n",
      "      - -389.0\n",
      "      - -21.0\n",
      "      - -194.0\n",
      "      - -32.0\n",
      "      - -398.0\n",
      "      - -317.0\n",
      "      - -461.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07014011990417605\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14686249145600067\n",
      "      mean_inference_ms: 1.3019433500487048\n",
      "      mean_raw_obs_processing_ms: 0.20525897084483571\n",
      "  time_since_restore: 268.241024017334\n",
      "  time_this_iter_s: 20.284728050231934\n",
      "  time_total_s: 268.241024017334\n",
      "  timers:\n",
      "    learn_throughput: 232.667\n",
      "    learn_time_ms: 17191.955\n",
      "    load_throughput: 675751.324\n",
      "    load_time_ms: 5.919\n",
      "    training_iteration_time_ms: 19124.033\n",
      "    update_time_ms: 5.661\n",
      "  timestamp: 1657259704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:55:09 (running for 00:04:50.13)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         268.241</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  -435.8</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -812</td><td style=\"text-align: right;\">            177.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:55:14 (running for 00:04:55.18)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         268.241</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  -435.8</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -812</td><td style=\"text-align: right;\">            177.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:55:19 (running for 00:05:00.19)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         268.241</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  -435.8</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -812</td><td style=\"text-align: right;\">            177.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:55:24 (running for 00:05:05.21)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/40 CPUs, 2.0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>RUNNING </td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         268.241</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  -435.8</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -812</td><td style=\"text-align: right;\">            177.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Taxi-v3_d9332_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_env_steps_sampled: 60000\n",
      "    num_env_steps_trained: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-08_07-55-24\n",
      "  done: true\n",
      "  episode_len_mean: 177.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -7.0\n",
      "  episode_reward_mean: -404.75\n",
      "  episode_reward_min: -776.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 320\n",
      "  experiment_id: 7b251ced4d844a0fb8570d8f93aed46f\n",
      "  hostname: sassauna2.ee.ethz.ch\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.2126518983353851\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01616111890676137\n",
      "          policy_loss: -0.040561869820599915\n",
      "          total_loss: 9.644420728375835\n",
      "          vf_explained_var: -0.04814397872135203\n",
      "          vf_loss: 9.680134253348074\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_env_steps_sampled: 60000\n",
      "    num_env_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 129.132.4.155\n",
      "  num_agent_steps_sampled: 60000\n",
      "  num_agent_steps_trained: 60000\n",
      "  num_env_steps_sampled: 60000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 60000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.1448275862068975\n",
      "    ram_util_percent: 9.2\n",
      "  pid: 39908\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06999933031397389\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.146487368442355\n",
      "    mean_inference_ms: 1.2993466480548466\n",
      "    mean_raw_obs_processing_ms: 0.20543224327699933\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 177.92\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -7.0\n",
      "    episode_reward_mean: -404.75\n",
      "    episode_reward_min: -776.0\n",
      "    episodes_this_iter: 21\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 200\n",
      "      - 200\n",
      "      - 105\n",
      "      - 200\n",
      "      - 200\n",
      "      - 140\n",
      "      - 200\n",
      "      - 154\n",
      "      - 200\n",
      "      - 200\n",
      "      - 19\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 121\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 74\n",
      "      - 138\n",
      "      - 200\n",
      "      - 200\n",
      "      - 49\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 109\n",
      "      - 60\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 118\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 22\n",
      "      - 36\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 189\n",
      "      - 200\n",
      "      - 200\n",
      "      - 24\n",
      "      - 116\n",
      "      - 44\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 79\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 116\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 79\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      - 200\n",
      "      episode_reward:\n",
      "      - -515.0\n",
      "      - -776.0\n",
      "      - -192.0\n",
      "      - -524.0\n",
      "      - -749.0\n",
      "      - -236.0\n",
      "      - -380.0\n",
      "      - -412.0\n",
      "      - -677.0\n",
      "      - -560.0\n",
      "      - -7.0\n",
      "      - -551.0\n",
      "      - -677.0\n",
      "      - -353.0\n",
      "      - -578.0\n",
      "      - -380.0\n",
      "      - -280.0\n",
      "      - -542.0\n",
      "      - -560.0\n",
      "      - -290.0\n",
      "      - -152.0\n",
      "      - -387.0\n",
      "      - -641.0\n",
      "      - -470.0\n",
      "      - -55.0\n",
      "      - -515.0\n",
      "      - -623.0\n",
      "      - -560.0\n",
      "      - -178.0\n",
      "      - -111.0\n",
      "      - -686.0\n",
      "      - -542.0\n",
      "      - -542.0\n",
      "      - -587.0\n",
      "      - -268.0\n",
      "      - -398.0\n",
      "      - -605.0\n",
      "      - -452.0\n",
      "      - -587.0\n",
      "      - -623.0\n",
      "      - -308.0\n",
      "      - -317.0\n",
      "      - -668.0\n",
      "      - -479.0\n",
      "      - -443.0\n",
      "      - -479.0\n",
      "      - -434.0\n",
      "      - -749.0\n",
      "      - -506.0\n",
      "      - -506.0\n",
      "      - -479.0\n",
      "      - -533.0\n",
      "      - -371.0\n",
      "      - -587.0\n",
      "      - -380.0\n",
      "      - -506.0\n",
      "      - -281.0\n",
      "      - -272.0\n",
      "      - -479.0\n",
      "      - -344.0\n",
      "      - -10.0\n",
      "      - -60.0\n",
      "      - -425.0\n",
      "      - -425.0\n",
      "      - -578.0\n",
      "      - -317.0\n",
      "      - -236.0\n",
      "      - -272.0\n",
      "      - -407.0\n",
      "      - -317.0\n",
      "      - -420.0\n",
      "      - -290.0\n",
      "      - -389.0\n",
      "      - -21.0\n",
      "      - -194.0\n",
      "      - -32.0\n",
      "      - -398.0\n",
      "      - -317.0\n",
      "      - -461.0\n",
      "      - -380.0\n",
      "      - -434.0\n",
      "      - -184.0\n",
      "      - -470.0\n",
      "      - -407.0\n",
      "      - -272.0\n",
      "      - -158.0\n",
      "      - -335.0\n",
      "      - -569.0\n",
      "      - -668.0\n",
      "      - -371.0\n",
      "      - -103.0\n",
      "      - -362.0\n",
      "      - -281.0\n",
      "      - -479.0\n",
      "      - -407.0\n",
      "      - -434.0\n",
      "      - -326.0\n",
      "      - -371.0\n",
      "      - -254.0\n",
      "      - -299.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.06999933031397389\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.146487368442355\n",
      "      mean_inference_ms: 1.2993466480548466\n",
      "      mean_raw_obs_processing_ms: 0.20543224327699933\n",
      "  time_since_restore: 288.31346011161804\n",
      "  time_this_iter_s: 20.072436094284058\n",
      "  time_total_s: 288.31346011161804\n",
      "  timers:\n",
      "    learn_throughput: 232.59\n",
      "    learn_time_ms: 17197.644\n",
      "    load_throughput: 677002.938\n",
      "    load_time_ms: 5.908\n",
      "    training_iteration_time_ms: 19124.606\n",
      "    update_time_ms: 5.683\n",
      "  timestamp: 1657259724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: d9332_00000\n",
      "  warmup_time: 8.1010901927948\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>agent_timesteps_total</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>counters/num_agent_steps_sampled</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>counters/num_agent_steps_trained</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>counters/num_env_steps_sampled</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>counters/num_env_steps_trained</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>episode_len_mean</td><td>▆██▇▆▇▆▆▆▅▃▁▂▁▂</td></tr><tr><td>episode_reward_max</td><td>▁▁▁▁████▄▄█████</td></tr><tr><td>episode_reward_mean</td><td>▂▁▁▁▂▂▃▃▄▄▅▆▆▇█</td></tr><tr><td>episode_reward_min</td><td>▄▁▁▁▁▁▂▂▂▂▅▇▇▇█</td></tr><tr><td>episodes_this_iter</td><td>▁▁▁▁▅▁▂▁▄▂▇█▁▅▂</td></tr><tr><td>episodes_total</td><td>▁▁▂▂▃▃▄▄▅▅▆▇▇██</td></tr><tr><td>info/learner/default_policy/learner_stats/cur_kl_coeff</td><td>▁▁▁▁▁▁▁▁▁▁█████</td></tr><tr><td>info/learner/default_policy/learner_stats/cur_lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>info/learner/default_policy/learner_stats/entropy</td><td>██▇▇▇▆▅▆▅▄▄▃▃▁▂</td></tr><tr><td>info/learner/default_policy/learner_stats/entropy_coeff</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>info/learner/default_policy/learner_stats/kl</td><td>▁▄▄▅▆▆▅▇▆█▇▇▇▅▆</td></tr><tr><td>info/learner/default_policy/learner_stats/policy_loss</td><td>█▄▃▂▁▂▃▁▂▂▂▂▄▃▃</td></tr><tr><td>info/learner/default_policy/learner_stats/total_loss</td><td>█▇▆▅▄▅▅▅▃▃▃▂▃▁▁</td></tr><tr><td>info/learner/default_policy/learner_stats/vf_explained_var</td><td>████▇▇█▇▇▆█▇▃▃▁</td></tr><tr><td>info/learner/default_policy/learner_stats/vf_loss</td><td>█▇▇▅▅▆▅▆▄▃▃▂▃▁▁</td></tr><tr><td>info/num_agent_steps_sampled</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>info/num_agent_steps_trained</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>info/num_env_steps_sampled</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>info/num_env_steps_trained</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>iterations_since_restore</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>num_agent_steps_sampled</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>num_agent_steps_trained</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>num_env_steps_sampled</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>num_env_steps_sampled_this_iter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>num_env_steps_trained</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>num_env_steps_trained_this_iter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>num_healthy_workers</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/cpu_util_percent</td><td>▁▃▄▄▃▂▁▃▅█▅▃▄▃▃</td></tr><tr><td>perf/ram_util_percent</td><td>▁██████████████</td></tr><tr><td>sampler_perf/mean_action_processing_ms</td><td>█▆▅▄▄▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>sampler_perf/mean_env_render_ms</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sampler_perf/mean_env_wait_ms</td><td>█▆▅▄▄▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>sampler_perf/mean_inference_ms</td><td>▅▁▂▂▄▄▆▇▇▇█▇▇▅▄</td></tr><tr><td>sampler_perf/mean_raw_obs_processing_ms</td><td>█▅▃▂▂▁▁▁▂▂▂▃▃▃▃</td></tr><tr><td>sampler_results/episode_len_mean</td><td>▆██▇▆▇▆▆▆▅▃▁▂▁▂</td></tr><tr><td>sampler_results/episode_reward_max</td><td>▁▁▁▁████▄▄█████</td></tr><tr><td>sampler_results/episode_reward_mean</td><td>▂▁▁▁▂▂▃▃▄▄▅▆▆▇█</td></tr><tr><td>sampler_results/episode_reward_min</td><td>▄▁▁▁▁▁▂▂▂▂▅▇▇▇█</td></tr><tr><td>sampler_results/episodes_this_iter</td><td>▁▁▁▁▅▁▂▁▄▂▇█▁▅▂</td></tr><tr><td>sampler_results/sampler_perf/mean_action_processing_ms</td><td>█▆▅▄▄▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>sampler_results/sampler_perf/mean_env_render_ms</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sampler_results/sampler_perf/mean_env_wait_ms</td><td>█▆▅▄▄▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>sampler_results/sampler_perf/mean_inference_ms</td><td>▅▁▂▂▄▄▆▇▇▇█▇▇▅▄</td></tr><tr><td>sampler_results/sampler_perf/mean_raw_obs_processing_ms</td><td>█▅▃▂▂▁▁▁▂▂▂▃▃▃▃</td></tr><tr><td>time_since_restore</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>time_this_iter_s</td><td>█▃▇▅▇▅▇▄▇▁▆▆▇▇▇</td></tr><tr><td>time_total_s</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>timers/learn_throughput</td><td>▁█▆▆▆▆▆▆▆▇█▇▇▇▇</td></tr><tr><td>timers/learn_time_ms</td><td>█▁▃▃▃▃▃▂▃▂▁▂▂▂▂</td></tr><tr><td>timers/load_throughput</td><td>▄█▇▄▃▄▃▃▂▃▂▂▁▁▁</td></tr><tr><td>timers/load_time_ms</td><td>▅▁▂▄▅▅▆▆▇▆▇▇███</td></tr><tr><td>timers/training_iteration_time_ms</td><td>█▁▃▂▃▃▃▂▃▁▁▂▂▂▂</td></tr><tr><td>timers/update_time_ms</td><td>▂▁▄█▇▇▆▇▇▆▆▇▇▆▆</td></tr><tr><td>timestamp</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>timesteps_since_restore</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>timesteps_total</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>training_iteration</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>warmup_time</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>agent_timesteps_total</td><td>60000</td></tr><tr><td>counters/num_agent_steps_sampled</td><td>60000</td></tr><tr><td>counters/num_agent_steps_trained</td><td>60000</td></tr><tr><td>counters/num_env_steps_sampled</td><td>60000</td></tr><tr><td>counters/num_env_steps_trained</td><td>60000</td></tr><tr><td>episode_len_mean</td><td>177.92</td></tr><tr><td>episode_reward_max</td><td>-7.0</td></tr><tr><td>episode_reward_mean</td><td>-404.75</td></tr><tr><td>episode_reward_min</td><td>-776.0</td></tr><tr><td>episodes_this_iter</td><td>21</td></tr><tr><td>episodes_total</td><td>320</td></tr><tr><td>info/learner/default_policy/learner_stats/cur_kl_coeff</td><td>0.3</td></tr><tr><td>info/learner/default_policy/learner_stats/cur_lr</td><td>5e-05</td></tr><tr><td>info/learner/default_policy/learner_stats/entropy</td><td>1.21265</td></tr><tr><td>info/learner/default_policy/learner_stats/entropy_coeff</td><td>0.0</td></tr><tr><td>info/learner/default_policy/learner_stats/kl</td><td>0.01616</td></tr><tr><td>info/learner/default_policy/learner_stats/policy_loss</td><td>-0.04056</td></tr><tr><td>info/learner/default_policy/learner_stats/total_loss</td><td>9.64442</td></tr><tr><td>info/learner/default_policy/learner_stats/vf_explained_var</td><td>-0.04814</td></tr><tr><td>info/learner/default_policy/learner_stats/vf_loss</td><td>9.68013</td></tr><tr><td>info/num_agent_steps_sampled</td><td>60000</td></tr><tr><td>info/num_agent_steps_trained</td><td>60000</td></tr><tr><td>info/num_env_steps_sampled</td><td>60000</td></tr><tr><td>info/num_env_steps_trained</td><td>60000</td></tr><tr><td>iterations_since_restore</td><td>15</td></tr><tr><td>num_agent_steps_sampled</td><td>60000</td></tr><tr><td>num_agent_steps_trained</td><td>60000</td></tr><tr><td>num_env_steps_sampled</td><td>60000</td></tr><tr><td>num_env_steps_sampled_this_iter</td><td>4000</td></tr><tr><td>num_env_steps_trained</td><td>60000</td></tr><tr><td>num_env_steps_trained_this_iter</td><td>4000</td></tr><tr><td>num_healthy_workers</td><td>4</td></tr><tr><td>perf/cpu_util_percent</td><td>5.14483</td></tr><tr><td>perf/ram_util_percent</td><td>9.2</td></tr><tr><td>sampler_perf/mean_action_processing_ms</td><td>0.07</td></tr><tr><td>sampler_perf/mean_env_render_ms</td><td>0.0</td></tr><tr><td>sampler_perf/mean_env_wait_ms</td><td>0.14649</td></tr><tr><td>sampler_perf/mean_inference_ms</td><td>1.29935</td></tr><tr><td>sampler_perf/mean_raw_obs_processing_ms</td><td>0.20543</td></tr><tr><td>sampler_results/episode_len_mean</td><td>177.92</td></tr><tr><td>sampler_results/episode_reward_max</td><td>-7.0</td></tr><tr><td>sampler_results/episode_reward_mean</td><td>-404.75</td></tr><tr><td>sampler_results/episode_reward_min</td><td>-776.0</td></tr><tr><td>sampler_results/episodes_this_iter</td><td>21</td></tr><tr><td>sampler_results/sampler_perf/mean_action_processing_ms</td><td>0.07</td></tr><tr><td>sampler_results/sampler_perf/mean_env_render_ms</td><td>0.0</td></tr><tr><td>sampler_results/sampler_perf/mean_env_wait_ms</td><td>0.14649</td></tr><tr><td>sampler_results/sampler_perf/mean_inference_ms</td><td>1.29935</td></tr><tr><td>sampler_results/sampler_perf/mean_raw_obs_processing_ms</td><td>0.20543</td></tr><tr><td>time_since_restore</td><td>288.31346</td></tr><tr><td>time_this_iter_s</td><td>20.07244</td></tr><tr><td>time_total_s</td><td>288.31346</td></tr><tr><td>timers/learn_throughput</td><td>232.59</td></tr><tr><td>timers/learn_time_ms</td><td>17197.644</td></tr><tr><td>timers/load_throughput</td><td>677002.938</td></tr><tr><td>timers/load_time_ms</td><td>5.908</td></tr><tr><td>timers/training_iteration_time_ms</td><td>19124.606</td></tr><tr><td>timers/update_time_ms</td><td>5.683</td></tr><tr><td>timestamp</td><td>1657259724</td></tr><tr><td>timesteps_since_restore</td><td>0</td></tr><tr><td>timesteps_total</td><td>60000</td></tr><tr><td>training_iteration</td><td>15</td></tr><tr><td>warmup_time</td><td>8.10109</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">PPO_Taxi-v3_d9332_00000</strong>: <a href=\"https://wandb.ai/danky/Taxi_v3/runs/d9332_00000\" target=\"_blank\">https://wandb.ai/danky/Taxi_v3/runs/d9332_00000</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220708_075019-d9332_00000/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-08 07:55:28 (running for 00:05:09.52)<br>Memory usage on this node: 11.5/125.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/40 CPUs, 0/4 GPUs, 0.0/74.75 GiB heap, 0.0/36.03 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/sem22h2/Documents/ExploringRL/logs/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Taxi-v3_d9332_00000</td><td>TERMINATED</td><td>129.132.4.155:39908</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         288.313</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> -404.75</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                -776</td><td style=\"text-align: right;\">            177.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RolloutWorker pid=40009)\u001B[0m E0708 07:55:28.858038367   40113 chttp2_transport.cc:1111]   Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=40008)\u001B[0m E0708 07:55:28.856797478   40093 chttp2_transport.cc:1111]   Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "2022-07-08 07:55:28,966\tINFO tune.py:747 -- Total run time: 310.04 seconds (309.48 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fea179ad130>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SELECT_ENV = \"Taxi-v3\"\n",
    "N_ITER = 20\n",
    "\n",
    "\n",
    "ray.init()\n",
    "ray.tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"training_iteration\": 15},\n",
    "    config={\n",
    "        \"env\": SELECT_ENV,\n",
    "        \"record_env\": True,\n",
    "        \"framework\": \"torch\",\n",
    "        \"num_cpus_per_worker\": 2,\n",
    "        \"num_gpus\": 2,\n",
    "        \"num_workers\": 4,\n",
    "    },\n",
    "    local_dir=\"logs\",\n",
    "    callbacks=[\n",
    "            WandbLoggerCallback(api_key=\"c36c598399c6c7f2f0b446aac164da6c7956a263\", project=\"Taxi_v3\")],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "# or simply get the last checkpoint (with highest \"training_iteration\")\n",
    "last_checkpoint = analysis.get_last_checkpoint()\n",
    "# if there are multiple trials, select a specific trial or automatically\n",
    "# choose the best one according to a given metric\n",
    "last_checkpoint = analysis.get_last_checkpoint(\n",
    "    metric=\"episode_reward_mean\", mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Creaete NAS Gym env\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}