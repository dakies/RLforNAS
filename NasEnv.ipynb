{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d04a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from nasbench import api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(self, adjacency_mat, labeling, budget=108):\n",
    "    labeling = ['input'] + list(labeling) + ['output']\n",
    "    model_spec = api.ModelSpec(adjacency_mat, labeling)\n",
    "    try:\n",
    "        data = self.dataset.query(model_spec, epochs=budget)\n",
    "    except api.OutOfDomainError:\n",
    "        # self.record_invalid(adjacency_mat, labeling, 1, 1, 0)\n",
    "        return 0, 0\n",
    "\n",
    "    # self.record_valid(adjacency_mat, labeling, data, model_spec)\n",
    "    return data[\"validation_accuracy\"], data[\"training_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ff7ab9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gzm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/rw/_2pjhm4n5_3c7j_tdjhf3fs40000gn/T/ipykernel_5162/1699296093.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mclass\u001B[0m \u001B[0mNasBench\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgzm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEnv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0madjecency_mat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madjecency_mat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'gzm' is not defined"
     ]
    }
   ],
   "source": [
    "# Todo: sample architecture\n",
    "# Todo: Check architecture validity. Neg reward if not.\n",
    "\n",
    "class NasBench(gym.Env):\n",
    "    adjacency_mat = None\n",
    "    def __init__(self, v=7, e=9, ops=['conv1x1-bn-relu', 'conv3x3-bn-relu', 'maxpool3x3'], step_max=1000):\n",
    "        # Environment definition\n",
    "        self.max_edges = e\n",
    "        self.vertices = v\n",
    "        self.ops = ops\n",
    "\n",
    "        # Helper\n",
    "        self.idx_upper = np.triu_indices(self.vertices) # Indices of upper triangular matrix\n",
    "\n",
    "        # Current state\n",
    "        self.adjacency_mat = np.zeros([v,v])\n",
    "        self.labeling = (v-2)*[ops[0]] # Initialize op for all layers that are not input or output layer\n",
    "\n",
    "        self.num_step = 0\n",
    "        self.step_max =  step_max\n",
    "\n",
    "        \n",
    "    def step(self, action):\n",
    "        e=self.max_edges\n",
    "        v=self.vertices\n",
    "        n = (e*(e+1)/2) # Number of indices in upper triag. part of matrix\n",
    "        if action < n:\n",
    "            # Todo: Check this changes matrix at right place\n",
    "            iu = self.idx_upper\n",
    "            self.adjacency_mat[iu[action]] = not self.adjacency_mat[iu[action]]\n",
    "        else:\n",
    "            o=len(self.ops)\n",
    "            action = action - n\n",
    "            [label_row, op] = np.unravel_index(action,[v, o])\n",
    "            self.labeling[label_row] = op\n",
    "\n",
    "        y, c = objective_function(self.adjacency_mat, self.labeling)\n",
    "        reward = y\n",
    "        if self.step == self.step_max:\n",
    "            done = 1\n",
    "        else: done = 0\n",
    "\n",
    "        observation = None\n",
    "        info = None\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.adjacency_mat[self.idx_upper] = np.random.randint(0,2,len(self.idx_upper))\n",
    "        self.labeling = np.random.randint(0,3,len(self.labeling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Configure the algorithm.\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"Taxi-v3\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 2,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"torch\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create our RLlib Trainer.\n",
    "trainer = PPOTrainer(config=config)\n",
    "\n",
    "# Run it for n training iterations. A training iteration includes\n",
    "# parallel sample collection by the environment workers as well as\n",
    "# loss calculation on the collected batch and a model update.\n",
    "for _ in range(3):\n",
    "    print(trainer.train())\n",
    "\n",
    "# Evaluate the trained Trainer (and render each timestep to the shell's\n",
    "# output).\n",
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}